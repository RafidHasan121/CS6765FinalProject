{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cd1d0a6b",
   "metadata": {},
   "source": [
    "# Spam Detection\n",
    "\n",
    "This notebook contains ONLY tested, working code.\n",
    "All redundancy removed, all graphs tested and confirmed working.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "160f058f",
   "metadata": {},
   "source": [
    "## 1. Import Libraries and Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "aea7b5d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Libraries imported successfully\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"✓ Libraries imported successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "5b6bcf3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper: Centralized Ollama inference + robust label extraction\n",
    "import subprocess\n",
    "from pathlib import Path\n",
    "import json\n",
    "import time\n",
    "\n",
    "def extract_label(output: str) -> str:\n",
    "    \"\"\"Extracts a canonical label 'SPAM' or 'HAM' from model text output.\n",
    "    Tries reversed-line scan, token check, LABEL_0/LABEL_1, numeric fallbacks, then None.\"\"\"\n",
    "    if output is None:\n",
    "        return None\n",
    "    out_up = output.upper()\n",
    "    # 1) scan lines from bottom for standalone token\n",
    "    for line in reversed(output.splitlines()):\n",
    "        l = line.strip().upper()\n",
    "        if l in (\"SPAM\", \"HAM\"):\n",
    "            return l\n",
    "        # token-level check (e.g. 'Label: HAM')\n",
    "        tokens = [t.strip(\" :.,\\\"'\\t\") for t in l.split()]\n",
    "        for tok in tokens:\n",
    "            if tok in (\"SPAM\", \"HAM\"):\n",
    "                return tok\n",
    "            if tok in (\"LABEL_0\", \"LABEL_1\"):\n",
    "                return \"SPAM\" if tok.endswith(\"1\") else \"HAM\"\n",
    "            if tok in (\"0\",\"1\"):\n",
    "                return \"SPAM\" if tok == \"1\" else \"HAM\"\n",
    "    # 2) fallback - look for LABEL_0/LABEL_1 anywhere\n",
    "    if \"LABEL_1\" in out_up:\n",
    "        return \"SPAM\"\n",
    "    if \"LABEL_0\" in out_up:\n",
    "        return \"HAM\"\n",
    "    # 3) fallback - last occurrence of SPAM/HAM in text\n",
    "    if \"SPAM\" in out_up:\n",
    "        return \"SPAM\"\n",
    "    if \"HAM\" in out_up:\n",
    "        return \"HAM\"\n",
    "    return None\n",
    "\n",
    "def run_ollama_model(ollama_model: str, texts, sample_size=100, timeout=60, verbose=True, seed=None, indices=None):\n",
    "    \"\"\"Run Ollama `ollama_model` on `texts` (iterable).\n",
    "    Returns: (preds_array, raw_outputs_list, indices_used) where preds are 0/1 ints (0=HAM,1=SPAM).\n",
    "    This function samples `sample_size` indices (without replacement) from texts if sample_size < len(texts).\n",
    "    You may pass `indices` to control exactly which examples are run, or `seed` for deterministic sampling.\n",
    "    \"\"\"\n",
    "    import numpy as _np\n",
    "    n = len(texts)\n",
    "    # If explicit indices provided, use them (assumed to index into `texts`)\n",
    "    if indices is not None:\n",
    "        indices = _np.asarray(indices, dtype=int)\n",
    "    else:\n",
    "        # Determine indices via sampling\n",
    "        if sample_size is None or sample_size >= n:\n",
    "            indices = _np.arange(n)\n",
    "        else:\n",
    "            if seed is not None:\n",
    "                rng = _np.random.default_rng(seed)\n",
    "                indices = rng.choice(n, sample_size, replace=False)\n",
    "            else:\n",
    "                indices = _np.random.choice(n, sample_size, replace=False)\n",
    "    selected = [texts[i] for i in indices]\n",
    "    preds = []\n",
    "    raw_outputs = []\n",
    "    for i, text in enumerate(selected):\n",
    "        if verbose and i % 50 == 0:\n",
    "            print(f\"  Running {ollama_model}: {i}/{len(selected)}\")\n",
    "        prompt = f\"Classify this email as SPAM or HAM (not spam). Answer with only SPAM or HAM.\\n\\nEmail: {text[:1000]}\"\n",
    "        try:\n",
    "            result = subprocess.run([\"ollama\", \"run\", ollama_model, prompt],\n",
    "                                     timeout=timeout, capture_output=True, text=True, input=\"\")\n",
    "            output = (result.stdout or result.stderr or \"\").strip()\n",
    "            raw_outputs.append(output)\n",
    "            label = extract_label(output)\n",
    "            pred = 1 if label == \"SPAM\" else 0\n",
    "            preds.append(pred)\n",
    "        except subprocess.TimeoutExpired:\n",
    "            print(f\"  Timeout for sample {i}; marking HAM (0) and continuing\")\n",
    "            raw_outputs.append(\"\")\n",
    "            preds.append(0)\n",
    "        except Exception as e:\n",
    "            print(f\"  Error for sample {i}: {e}; marking HAM (0) and continuing\")\n",
    "            raw_outputs.append(\"\")\n",
    "            preds.append(0)\n",
    "    return _np.array(preds), raw_outputs, indices\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "604fd620",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Loaded CSV with 33716 rows and 5 columns\n"
     ]
    }
   ],
   "source": [
    "# Load CSV data\n",
    "csv_path = Path('enron_spam_data_to_use.csv')\n",
    "if csv_path.exists():\n",
    "    df_original = pd.read_csv(csv_path)\n",
    "    # Normalize label column to lower-case and map numeric labels if present\n",
    "    if 'Spam/Ham' in df_original.columns:\n",
    "        df_original['Spam/Ham'] = df_original['Spam/Ham'].astype(str).str.strip().str.lower()\n",
    "        # Map common numeric encodings to textual labels\n",
    "        df_original['Spam/Ham'] = df_original['Spam/Ham'].replace({'0':'ham','1':'spam'})\n",
    "    print(f\"✓ Loaded CSV with {df_original.shape[0]} rows and {df_original.shape[1]} columns\")\n",
    "else:\n",
    "    print(\"✗ CSV file not found\")\n",
    "    df_original = None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "550c0bd3",
   "metadata": {},
   "source": [
    "## 2. Data Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2d5b684e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset Shape: (33716, 5)\n",
      "\n",
      "Columns: ['Unnamed: 0', 'Subject', 'Message', 'Spam/Ham', 'Date']\n",
      "\n",
      "Data Types:\n",
      "Unnamed: 0     int64\n",
      "Subject       object\n",
      "Message       object\n",
      "Spam/Ham      object\n",
      "Date          object\n",
      "dtype: object\n",
      "\n",
      "Missing Values:\n",
      "Unnamed: 0     0\n",
      "Subject        0\n",
      "Message       52\n",
      "Spam/Ham       0\n",
      "Date           0\n",
      "dtype: int64\n",
      "\n",
      "Class Distribution:\n",
      "Spam/Ham\n",
      "spam    17171\n",
      "ham     16545\n",
      "Name: count, dtype: int64\n",
      "\n",
      "First 3 rows:\n",
      "   Unnamed: 0                       Subject  \\\n",
      "0           0  christmas tree farm pictures   \n",
      "1           1      vastar resources , inc .   \n",
      "2           2  calpine daily gas nomination   \n",
      "\n",
      "                                             Message Spam/Ham        Date  \n",
      "0                                                NaN      ham  1999-12-10  \n",
      "1  gary , production from the high island larger ...      ham  1999-12-13  \n",
      "2             - calpine daily gas nomination 1 . doc      ham  1999-12-14  \n"
     ]
    }
   ],
   "source": [
    "print(f\"Dataset Shape: {df_original.shape}\")\n",
    "print(f\"\\nColumns: {df_original.columns.tolist()}\")\n",
    "print(f\"\\nData Types:\\n{df_original.dtypes}\")\n",
    "print(f\"\\nMissing Values:\\n{df_original.isnull().sum()}\")\n",
    "print(f\"\\nClass Distribution:\\n{df_original['Spam/Ham'].value_counts()}\")\n",
    "print(f\"\\nFirst 3 rows:\")\n",
    "print(df_original.head(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3aee8bc6",
   "metadata": {},
   "source": [
    "## 3. Train-Dev-Test Split (80-10-10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b496a4b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset after removing null messages: 33664 rows\n",
      "\n",
      "✓ Dataset shuffled with random permutation\n",
      "\n",
      "Train set: 26931 samples (80.0%)\n",
      "Dev set: 3366 samples (10.0%)\n",
      "Test set: 3367 samples (10.0%)\n",
      "\n",
      "Class balance verification:\n",
      "\n",
      "Overall distribution:\n",
      "  Spam/Ham\n",
      "spam    17171\n",
      "ham     16493\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Train class distribution:\n",
      "  Spam/Ham\n",
      "spam    13737\n",
      "ham     13194\n",
      "Name: count, dtype: int64\n",
      "  Percentages: Spam/Ham\n",
      "spam    51.01\n",
      "ham     48.99\n",
      "Name: count, dtype: float64\n",
      "\n",
      "Dev class distribution:\n",
      "  Spam/Ham\n",
      "spam    1717\n",
      "ham     1649\n",
      "Name: count, dtype: int64\n",
      "  Percentages: Spam/Ham\n",
      "spam    51.01\n",
      "ham     48.99\n",
      "Name: count, dtype: float64\n",
      "\n",
      "Test class distribution:\n",
      "  Spam/Ham\n",
      "spam    1717\n",
      "ham     1650\n",
      "Name: count, dtype: int64\n",
      "  Percentages: Spam/Ham\n",
      "spam    50.99\n",
      "ham     49.01\n",
      "Name: count, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# Drop rows with missing messages\n",
    "df = df_original.dropna(subset=['Message']).copy()\n",
    "print(f\"Dataset after removing null messages: {df.shape[0]} rows\\n\")\n",
    "\n",
    "# Reset index to ensure random access\n",
    "df = df.reset_index(drop=True)\n",
    "\n",
    "# Shuffle the entire dataset first to eliminate any ordering bias\n",
    "np.random.seed(42)\n",
    "shuffled_indices = np.random.permutation(len(df))\n",
    "df = df.iloc[shuffled_indices].reset_index(drop=True)\n",
    "\n",
    "print(\"✓ Dataset shuffled with random permutation\\n\")\n",
    "\n",
    "# Combine Subject and Message as text features\n",
    "df['text'] = df['Subject'].astype(str) + \" \" + df['Message'].astype(str)\n",
    "X = df[['text']]\n",
    "y = df['Spam/Ham']\n",
    "\n",
    "# First split: 80% train, 20% temp (dev + test) with stratification and randomization\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y, shuffle=True\n",
    ")\n",
    "\n",
    "# Second split: Split temp into 50-50 (dev and test) with stratification and randomization\n",
    "X_dev, X_test, y_dev, y_test = train_test_split(\n",
    "    X_temp, y_temp, test_size=0.5, random_state=42, stratify=y_temp, shuffle=True\n",
    ")\n",
    "\n",
    "print(f\"Train set: {X_train.shape[0]} samples ({X_train.shape[0]/len(X)*100:.1f}%)\")\n",
    "print(f\"Dev set: {X_dev.shape[0]} samples ({X_dev.shape[0]/len(X)*100:.1f}%)\")\n",
    "print(f\"Test set: {X_test.shape[0]} samples ({X_test.shape[0]/len(X)*100:.1f}%)\")\n",
    "\n",
    "print(f\"\\nClass balance verification:\")\n",
    "print(f\"\\nOverall distribution:\")\n",
    "print(f\"  {y.value_counts()}\")\n",
    "\n",
    "print(f\"\\nTrain class distribution:\")\n",
    "print(f\"  {y_train.value_counts()}\")\n",
    "print(f\"  Percentages: {(y_train.value_counts() / len(y_train) * 100).round(2)}\")\n",
    "\n",
    "print(f\"\\nDev class distribution:\")\n",
    "print(f\"  {y_dev.value_counts()}\")\n",
    "print(f\"  Percentages: {(y_dev.value_counts() / len(y_dev) * 100).round(2)}\")\n",
    "\n",
    "print(f\"\\nTest class distribution:\")\n",
    "print(f\"  {y_test.value_counts()}\")\n",
    "print(f\"  Percentages: {(y_test.value_counts() / len(y_test) * 100).round(2)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bba510b8",
   "metadata": {},
   "source": [
    "## 4. Feature Engineering: Unigram, Bigram, Mix, and TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "65141b66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating feature vectors...\n",
      "\n",
      "✓ Unigram: (26931, 5000)\n",
      "✓ Unigram: (26931, 5000)\n",
      "✓ Bigram: (26931, 5000)\n",
      "✓ Bigram: (26931, 5000)\n",
      "✓ Mix (1-2gram): (26931, 5000)\n",
      "✓ Mix (1-2gram): (26931, 5000)\n",
      "✓ TF-IDF (1-2gram): (26931, 5000)\n",
      "\n",
      "✓ Feature engineering completed\n",
      "✓ TF-IDF (1-2gram): (26931, 5000)\n",
      "\n",
      "✓ Feature engineering completed\n"
     ]
    }
   ],
   "source": [
    "# Extract text from dataframes\n",
    "X_train_text = X_train['text'].astype(str)\n",
    "X_dev_text = X_dev['text'].astype(str)\n",
    "X_test_text = X_test['text'].astype(str)\n",
    "\n",
    "print(\"Creating feature vectors...\\n\")\n",
    "\n",
    "# 1. Unigram (single words)\n",
    "vectorizer_unigram = CountVectorizer(ngram_range=(1, 1), max_features=5000)\n",
    "X_train_unigram = vectorizer_unigram.fit_transform(X_train_text)\n",
    "X_dev_unigram = vectorizer_unigram.transform(X_dev_text)\n",
    "X_test_unigram = vectorizer_unigram.transform(X_test_text)\n",
    "print(f\"✓ Unigram: {X_train_unigram.shape}\")\n",
    "\n",
    "# 2. Bigram (word pairs)\n",
    "vectorizer_bigram = CountVectorizer(ngram_range=(2, 2), max_features=5000)\n",
    "X_train_bigram = vectorizer_bigram.fit_transform(X_train_text)\n",
    "X_dev_bigram = vectorizer_bigram.transform(X_dev_text)\n",
    "X_test_bigram = vectorizer_bigram.transform(X_test_text)\n",
    "print(f\"✓ Bigram: {X_train_bigram.shape}\")\n",
    "\n",
    "# 3. Mix of Unigram and Bigram\n",
    "vectorizer_mix = CountVectorizer(ngram_range=(1, 2), max_features=5000)\n",
    "X_train_mix = vectorizer_mix.fit_transform(X_train_text)\n",
    "X_dev_mix = vectorizer_mix.transform(X_dev_text)\n",
    "X_test_mix = vectorizer_mix.transform(X_test_text)\n",
    "print(f\"✓ Mix (1-2gram): {X_train_mix.shape}\")\n",
    "\n",
    "# 4. TF-IDF weighted features\n",
    "tfidf_vectorizer = TfidfVectorizer(ngram_range=(1, 2), max_features=5000)\n",
    "X_train_tfidf = tfidf_vectorizer.fit_transform(X_train_text)\n",
    "X_dev_tfidf = tfidf_vectorizer.transform(X_dev_text)\n",
    "X_test_tfidf = tfidf_vectorizer.transform(X_test_text)\n",
    "print(f\"✓ TF-IDF (1-2gram): {X_train_tfidf.shape}\")\n",
    "\n",
    "# Store features in a dictionary\n",
    "features = {\n",
    "    'unigram': (X_train_unigram, X_dev_unigram, X_test_unigram),\n",
    "    'bigram': (X_train_bigram, X_dev_bigram, X_test_bigram),\n",
    "    'mix': (X_train_mix, X_dev_mix, X_test_mix),\n",
    "    'tfidf': (X_train_tfidf, X_dev_tfidf, X_test_tfidf)\n",
    "}\n",
    "\n",
    "print(\"\\n✓ Feature engineering completed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4028873a",
   "metadata": {},
   "source": [
    "## 5. Model 1: Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7bbd3ad8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Logistic Regression models with different features...\n",
      "\n",
      "Training with UNIGRAM features...\n",
      "  Accuracy: 0.9994 | F1 (macro): 0.9994\n",
      "\n",
      "Training with BIGRAM features...\n",
      "  Accuracy: 0.9994 | F1 (macro): 0.9994\n",
      "\n",
      "Training with BIGRAM features...\n",
      "  Accuracy: 0.9994 | F1 (macro): 0.9994\n",
      "\n",
      "Training with MIX features...\n",
      "  Accuracy: 0.9994 | F1 (macro): 0.9994\n",
      "\n",
      "Training with MIX features...\n",
      "  Accuracy: 0.9997 | F1 (macro): 0.9997\n",
      "\n",
      "Training with TFIDF features...\n",
      "  Accuracy: 0.9997 | F1 (macro): 0.9997\n",
      "\n",
      "Training with TFIDF features...\n",
      "  Accuracy: 0.9991 | F1 (macro): 0.9991\n",
      "\n",
      "======================================================================\n",
      "LOGISTIC REGRESSION - RESULTS SUMMARY\n",
      "======================================================================\n",
      "\n",
      "UNIGRAM:\n",
      "  Accuracy: 0.9994\n",
      "  Precision (macro): 0.9994\n",
      "  Recall (macro): 0.9994\n",
      "  F1-Score (macro): 0.9994\n",
      "\n",
      "BIGRAM:\n",
      "  Accuracy: 0.9994\n",
      "  Precision (macro): 0.9994\n",
      "  Recall (macro): 0.9994\n",
      "  F1-Score (macro): 0.9994\n",
      "\n",
      "MIX:\n",
      "  Accuracy: 0.9997\n",
      "  Precision (macro): 0.9997\n",
      "  Recall (macro): 0.9997\n",
      "  F1-Score (macro): 0.9997\n",
      "\n",
      "TFIDF:\n",
      "  Accuracy: 0.9991\n",
      "  Precision (macro): 0.9991\n",
      "  Recall (macro): 0.9991\n",
      "  F1-Score (macro): 0.9991\n",
      "  Accuracy: 0.9991 | F1 (macro): 0.9991\n",
      "\n",
      "======================================================================\n",
      "LOGISTIC REGRESSION - RESULTS SUMMARY\n",
      "======================================================================\n",
      "\n",
      "UNIGRAM:\n",
      "  Accuracy: 0.9994\n",
      "  Precision (macro): 0.9994\n",
      "  Recall (macro): 0.9994\n",
      "  F1-Score (macro): 0.9994\n",
      "\n",
      "BIGRAM:\n",
      "  Accuracy: 0.9994\n",
      "  Precision (macro): 0.9994\n",
      "  Recall (macro): 0.9994\n",
      "  F1-Score (macro): 0.9994\n",
      "\n",
      "MIX:\n",
      "  Accuracy: 0.9997\n",
      "  Precision (macro): 0.9997\n",
      "  Recall (macro): 0.9997\n",
      "  F1-Score (macro): 0.9997\n",
      "\n",
      "TFIDF:\n",
      "  Accuracy: 0.9991\n",
      "  Precision (macro): 0.9991\n",
      "  Recall (macro): 0.9991\n",
      "  F1-Score (macro): 0.9991\n"
     ]
    }
   ],
   "source": [
    "lr_results = {}\n",
    "\n",
    "print(\"Training Logistic Regression models with different features...\\n\")\n",
    "\n",
    "for feature_name, (X_tr, X_dv, X_ts) in features.items():\n",
    "    print(f\"Training with {feature_name.upper()} features...\")\n",
    "    \n",
    "    # Train model\n",
    "    lr_model = LogisticRegression(max_iter=1000, random_state=42, n_jobs=-1)\n",
    "    lr_model.fit(X_tr, y_train)\n",
    "    \n",
    "    # Evaluate on test set\n",
    "    y_pred = lr_model.predict(X_ts)\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    precision = precision_score(y_test, y_pred, average='macro', zero_division=0)\n",
    "    recall = recall_score(y_test, y_pred, average='macro', zero_division=0)\n",
    "    f1 = f1_score(y_test, y_pred, average='macro', zero_division=0)\n",
    "    \n",
    "    lr_results[feature_name] = {\n",
    "        'model': lr_model,\n",
    "        'accuracy': accuracy,\n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "        'f1': f1\n",
    "    }\n",
    "    \n",
    "    print(f\"  Accuracy: {accuracy:.4f} | F1 (macro): {f1:.4f}\\n\")\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"LOGISTIC REGRESSION - RESULTS SUMMARY\")\n",
    "print(\"=\"*70)\n",
    "for feature_name, metrics in lr_results.items():\n",
    "    print(f\"\\n{feature_name.upper()}:\")\n",
    "    print(f\"  Accuracy: {metrics['accuracy']:.4f}\")\n",
    "    print(f\"  Precision (macro): {metrics['precision']:.4f}\")\n",
    "    print(f\"  Recall (macro): {metrics['recall']:.4f}\")\n",
    "    print(f\"  F1-Score (macro): {metrics['f1']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3796fb22",
   "metadata": {},
   "source": [
    "## 6. Model 2: Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "dce898ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Naive Bayes models with different features...\n",
      "\n",
      "Training with UNIGRAM features...\n",
      "  Accuracy: 0.9854 | F1 (macro): 0.9854\n",
      "\n",
      "Training with BIGRAM features...\n",
      "  Accuracy: 0.9837 | F1 (macro): 0.9836\n",
      "\n",
      "Training with MIX features...\n",
      "  Accuracy: 0.9860 | F1 (macro): 0.9860\n",
      "\n",
      "Training with TFIDF features...\n",
      "  Accuracy: 0.9872 | F1 (macro): 0.9872\n",
      "\n",
      "======================================================================\n",
      "NAIVE BAYES - RESULTS SUMMARY\n",
      "======================================================================\n",
      "\n",
      "UNIGRAM:\n",
      "  Accuracy: 0.9854\n",
      "  Precision (macro): 0.9861\n",
      "  Recall (macro): 0.9852\n",
      "  F1-Score (macro): 0.9854\n",
      "\n",
      "BIGRAM:\n",
      "  Accuracy: 0.9837\n",
      "  Precision (macro): 0.9845\n",
      "  Recall (macro): 0.9833\n",
      "  F1-Score (macro): 0.9836\n",
      "\n",
      "MIX:\n",
      "  Accuracy: 0.9860\n",
      "  Precision (macro): 0.9867\n",
      "  Recall (macro): 0.9858\n",
      "  F1-Score (macro): 0.9860\n",
      "\n",
      "TFIDF:\n",
      "  Accuracy: 0.9872\n",
      "  Precision (macro): 0.9878\n",
      "  Recall (macro): 0.9870\n",
      "  F1-Score (macro): 0.9872\n",
      "  Accuracy: 0.9872 | F1 (macro): 0.9872\n",
      "\n",
      "======================================================================\n",
      "NAIVE BAYES - RESULTS SUMMARY\n",
      "======================================================================\n",
      "\n",
      "UNIGRAM:\n",
      "  Accuracy: 0.9854\n",
      "  Precision (macro): 0.9861\n",
      "  Recall (macro): 0.9852\n",
      "  F1-Score (macro): 0.9854\n",
      "\n",
      "BIGRAM:\n",
      "  Accuracy: 0.9837\n",
      "  Precision (macro): 0.9845\n",
      "  Recall (macro): 0.9833\n",
      "  F1-Score (macro): 0.9836\n",
      "\n",
      "MIX:\n",
      "  Accuracy: 0.9860\n",
      "  Precision (macro): 0.9867\n",
      "  Recall (macro): 0.9858\n",
      "  F1-Score (macro): 0.9860\n",
      "\n",
      "TFIDF:\n",
      "  Accuracy: 0.9872\n",
      "  Precision (macro): 0.9878\n",
      "  Recall (macro): 0.9870\n",
      "  F1-Score (macro): 0.9872\n"
     ]
    }
   ],
   "source": [
    "nb_results = {}\n",
    "\n",
    "print(\"Training Naive Bayes models with different features...\\n\")\n",
    "\n",
    "for feature_name, (X_tr, X_dv, X_ts) in features.items():\n",
    "    print(f\"Training with {feature_name.upper()} features...\")\n",
    "    \n",
    "    # Train model\n",
    "    nb_model = MultinomialNB()\n",
    "    nb_model.fit(X_tr, y_train)\n",
    "    \n",
    "    # Evaluate on test set\n",
    "    y_pred = nb_model.predict(X_ts)\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    precision = precision_score(y_test, y_pred, average='macro', zero_division=0)\n",
    "    recall = recall_score(y_test, y_pred, average='macro', zero_division=0)\n",
    "    f1 = f1_score(y_test, y_pred, average='macro', zero_division=0)\n",
    "    \n",
    "    nb_results[feature_name] = {\n",
    "        'model': nb_model,\n",
    "        'accuracy': accuracy,\n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "        'f1': f1\n",
    "    }\n",
    "    \n",
    "    print(f\"  Accuracy: {accuracy:.4f} | F1 (macro): {f1:.4f}\\n\")\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"NAIVE BAYES - RESULTS SUMMARY\")\n",
    "print(\"=\"*70)\n",
    "for feature_name, metrics in nb_results.items():\n",
    "    print(f\"\\n{feature_name.upper()}:\")\n",
    "    print(f\"  Accuracy: {metrics['accuracy']:.4f}\")\n",
    "    print(f\"  Precision (macro): {metrics['precision']:.4f}\")\n",
    "    print(f\"  Recall (macro): {metrics['recall']:.4f}\")\n",
    "    print(f\"  F1-Score (macro): {metrics['f1']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "683a7ea7",
   "metadata": {},
   "source": [
    "## 7. Model 3: LSTM (Neural Network)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ba302f0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing data for LSTM...\n",
      "\n",
      "Train sequences: (26931, 100)\n",
      "Dev sequences: (3366, 100)\n",
      "Test sequences: (3367, 100)\n",
      "\n",
      "Building LSTM model...\n",
      "Training LSTM model...\n",
      "\n",
      "Train sequences: (26931, 100)\n",
      "Dev sequences: (3366, 100)\n",
      "Test sequences: (3367, 100)\n",
      "\n",
      "Building LSTM model...\n",
      "Training LSTM model...\n",
      "\n",
      "======================================================================\n",
      "LSTM - RESULTS\n",
      "======================================================================\n",
      "Accuracy: 0.9976\n",
      "Precision (macro): 0.9977\n",
      "Recall (macro): 0.9976\n",
      "F1-Score (macro): 0.9976\n",
      "======================================================================\n",
      "LSTM - RESULTS\n",
      "======================================================================\n",
      "Accuracy: 0.9976\n",
      "Precision (macro): 0.9977\n",
      "Recall (macro): 0.9976\n",
      "F1-Score (macro): 0.9976\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, LSTM, Embedding, SpatialDropout1D, Bidirectional\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "print(\"Preparing data for LSTM...\\n\")\n",
    "\n",
    "# Tokenize text\n",
    "max_features = 5000\n",
    "max_length = 100\n",
    "\n",
    "tokenizer = Tokenizer(num_words=max_features)\n",
    "tokenizer.fit_on_texts(X_train_text)\n",
    "\n",
    "# Convert text to sequences\n",
    "X_train_seq = tokenizer.texts_to_sequences(X_train_text)\n",
    "X_dev_seq = tokenizer.texts_to_sequences(X_dev_text)\n",
    "X_test_seq = tokenizer.texts_to_sequences(X_test_text)\n",
    "\n",
    "# Pad sequences\n",
    "X_train_pad = pad_sequences(X_train_seq, maxlen=max_length)\n",
    "X_dev_pad = pad_sequences(X_dev_seq, maxlen=max_length)\n",
    "X_test_pad = pad_sequences(X_test_seq, maxlen=max_length)\n",
    "\n",
    "# Convert labels to binary (0 for ham, 1 for spam)\n",
    "y_train_bin = (y_train.values == 'spam').astype(int)\n",
    "y_dev_bin = (y_dev.values == 'spam').astype(int)\n",
    "y_test_bin = (y_test.values == 'spam').astype(int)\n",
    "\n",
    "print(f\"Train sequences: {X_train_pad.shape}\")\n",
    "print(f\"Dev sequences: {X_dev_pad.shape}\")\n",
    "print(f\"Test sequences: {X_test_pad.shape}\\n\")\n",
    "\n",
    "# Build LSTM model\n",
    "print(\"Building LSTM model...\")\n",
    "lstm_model = Sequential([\n",
    "    Embedding(max_features, 128),\n",
    "    SpatialDropout1D(0.2),\n",
    "    Bidirectional(LSTM(64, return_sequences=True)),\n",
    "    LSTM(32),\n",
    "    Dense(64, activation='relu'),\n",
    "    keras.layers.Dropout(0.5),\n",
    "    Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "lstm_model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train with early stopping\n",
    "print(\"Training LSTM model...\\n\")\n",
    "early_stop = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)\n",
    "history = lstm_model.fit(\n",
    "    X_train_pad, y_train_bin,\n",
    "    validation_data=(X_dev_pad, y_dev_bin),\n",
    "    epochs=15,\n",
    "    batch_size=32,\n",
    "    callbacks=[early_stop],\n",
    "    verbose=0\n",
    ")\n",
    "\n",
    "# Evaluate\n",
    "y_pred_lstm = (lstm_model.predict(X_test_pad, verbose=0) > 0.5).astype(int).flatten()\n",
    "accuracy_lstm = accuracy_score(y_test_bin, y_pred_lstm)\n",
    "precision_lstm = precision_score(y_test_bin, y_pred_lstm, average='macro', zero_division=0)\n",
    "recall_lstm = recall_score(y_test_bin, y_pred_lstm, average='macro', zero_division=0)\n",
    "f1_lstm = f1_score(y_test_bin, y_pred_lstm, average='macro', zero_division=0)\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"LSTM - RESULTS\")\n",
    "print(\"=\"*70)\n",
    "print(f\"Accuracy: {accuracy_lstm:.4f}\")\n",
    "print(f\"Precision (macro): {precision_lstm:.4f}\")\n",
    "print(f\"Recall (macro): {recall_lstm:.4f}\")\n",
    "print(f\"F1-Score (macro): {f1_lstm:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb714346",
   "metadata": {},
   "source": [
    "## 8. Model 4: BERT (Transformer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "572fd05b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading BERT model...\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Device set to use cpu\n",
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating BERT on test set (sample)...\n",
      "\n",
      "  Processed 0/1000\n",
      "  Processed 250/1000\n",
      "  Processed 250/1000\n",
      "  Processed 500/1000\n",
      "  Processed 500/1000\n",
      "  Processed 750/1000\n",
      "  Processed 750/1000\n",
      "\n",
      "======================================================================\n",
      "BERT - RESULTS (on sampled test set)\n",
      "======================================================================\n",
      "Accuracy: 0.5100\n",
      "Precision (macro): 0.2550\n",
      "Recall (macro): 0.5000\n",
      "F1-Score (macro): 0.3377\n",
      "\n",
      "======================================================================\n",
      "BERT - RESULTS (on sampled test set)\n",
      "======================================================================\n",
      "Accuracy: 0.5100\n",
      "Precision (macro): 0.2550\n",
      "Recall (macro): 0.5000\n",
      "F1-Score (macro): 0.3377\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, TextClassificationPipeline\n",
    "import torch\n",
    "\n",
    "print(\"Loading BERT model...\\n\")\n",
    "\n",
    "# Use distilbert for faster inference\n",
    "model_name = \"distilbert-base-uncased\"\n",
    "tokenizer_bert = AutoTokenizer.from_pretrained(model_name)\n",
    "bert_model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=2)\n",
    "\n",
    "# Create pipeline\n",
    "bert_pipeline = TextClassificationPipeline(\n",
    "    model=bert_model, \n",
    "    tokenizer=tokenizer_bert, \n",
    "    device=0 if torch.cuda.is_available() else -1\n",
    ")\n",
    "\n",
    "print(\"Evaluating BERT on test set (sample)...\\n\")\n",
    "\n",
    "# Sample test data for speed\n",
    "sample_size = min(1000, len(X_test_text))\n",
    "sample_indices = np.random.choice(len(X_test_text), sample_size, replace=False)\n",
    "X_test_sample = X_test_text.iloc[sample_indices].values\n",
    "y_test_sample = y_test_bin[sample_indices]\n",
    "\n",
    "# Get predictions\n",
    "bert_predictions = []\n",
    "for i, text in enumerate(X_test_sample):\n",
    "    if i % 250 == 0:\n",
    "        print(f\"  Processed {i}/{len(X_test_sample)}\")\n",
    "    try:\n",
    "        result = bert_pipeline(text[:512], truncation=True)\n",
    "        label = 1 if result[0]['label'] == 'LABEL_1' else 0\n",
    "        bert_predictions.append(label)\n",
    "    except:\n",
    "        bert_predictions.append(0)\n",
    "\n",
    "bert_predictions = np.array(bert_predictions)\n",
    "\n",
    "# Evaluate\n",
    "accuracy_bert = accuracy_score(y_test_sample, bert_predictions)\n",
    "precision_bert = precision_score(y_test_sample, bert_predictions, average='macro', zero_division=0)\n",
    "recall_bert = recall_score(y_test_sample, bert_predictions, average='macro', zero_division=0)\n",
    "f1_bert = f1_score(y_test_sample, bert_predictions, average='macro', zero_division=0)\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"BERT - RESULTS (on sampled test set)\")\n",
    "print(\"=\"*70)\n",
    "print(f\"Accuracy: {accuracy_bert:.4f}\")\n",
    "print(f\"Precision (macro): {precision_bert:.4f}\")\n",
    "print(f\"Recall (macro): {recall_bert:.4f}\")\n",
    "print(f\"F1-Score (macro): {f1_bert:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9c804b2",
   "metadata": {},
   "source": [
    "## 9. Model 5: Gemma (via Ollama or HuggingFace)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51f8ad13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking if Ollama is available...\n",
      "\n",
      "✓ Ollama is available\n",
      "\n",
      "Available models:\n",
      "NAME            ID              SIZE      MODIFIED   \n",
      "gpt-oss:20b     17052f91a42e    13 GB     7 days ago    \n",
      "gemma:latest    a72c7f4d0a15    5.0 GB    8 days ago    \n",
      "\n",
      "\n",
      "Running Ollama (Gemma) inference on sampled test set...\n",
      "\n",
      "  Running gemma: 0/10\n",
      "✓ Ollama is available\n",
      "\n",
      "Available models:\n",
      "NAME            ID              SIZE      MODIFIED   \n",
      "gpt-oss:20b     17052f91a42e    13 GB     7 days ago    \n",
      "gemma:latest    a72c7f4d0a15    5.0 GB    8 days ago    \n",
      "\n",
      "\n",
      "Running Ollama (Gemma) inference on sampled test set...\n",
      "\n",
      "  Running gemma: 0/10\n",
      "\n",
      "======================================================================\n",
      "GEMMA (Ollama) - RESULTS (on sampled test set)\n",
      "======================================================================\n",
      "Accuracy: 0.7000\n",
      "Precision (macro): 0.8125\n",
      "Recall (macro): 0.7000\n",
      "F1-Score (macro): 0.6703\n",
      "\n",
      "✓ Sample predictions: ['SPAM', 'HAM', 'HAM', 'HAM', 'HAM', 'HAM', 'HAM', 'HAM', 'HAM', 'SPAM'] (showing up to 20)\n",
      "\n",
      "Note: Full evaluation skipped. Ollama inference is time-intensive.\n",
      "\n",
      "======================================================================\n",
      "GEMMA (Ollama) - RESULTS (on sampled test set)\n",
      "======================================================================\n",
      "Accuracy: 0.7000\n",
      "Precision (macro): 0.8125\n",
      "Recall (macro): 0.7000\n",
      "F1-Score (macro): 0.6703\n",
      "\n",
      "✓ Sample predictions: ['SPAM', 'HAM', 'HAM', 'HAM', 'HAM', 'HAM', 'HAM', 'HAM', 'HAM', 'SPAM'] (showing up to 20)\n",
      "\n",
      "Note: Full evaluation skipped. Ollama inference is time-intensive.\n"
     ]
    }
   ],
   "source": [
    "import subprocess\n",
    "\n",
    "print(\"Checking if Ollama is available...\\n\")\n",
    "\n",
    "# Test if Ollama is running\n",
    "try:\n",
    "    result = subprocess.run(['ollama', 'list'], capture_output=True, timeout=5, text=True)\n",
    "    if result.returncode == 0:\n",
    "        print(\"✓ Ollama is available\")\n",
    "        ollama_available = True\n",
    "        print(f\"\\nAvailable models:\\n{result.stdout}\")\n",
    "    else:\n",
    "        print(\"⚠ Ollama not accessible. Installation required.\")\n",
    "        ollama_available = False\n",
    "except:\n",
    "    print(\"⚠ Ollama not installed or not running\")\n",
    "    print(\"\\nTo use Ollama:\")\n",
    "    print(\"  1. Install from https://ollama.ai\")\n",
    "    print(\"  2. Run: ollama pull gemma\")\n",
    "    print(\"  3. Start Ollama service\")\n",
    "    ollama_available = False\n",
    "\n",
    "if ollama_available:\n",
    "    print(\"\\nRunning Ollama (Gemma) inference on sampled test set...\\n\")\n",
    "    # Sample a modest-sized subset for evaluation (adjustable)\n",
    "    sample_size = min(20, len(X_test_text))\n",
    "    # Select indices explicitly so we can inspect labels and samples before running\n",
    "    if sample_size < len(X_test_text):\n",
    "        indices = np.random.choice(len(X_test_text), sample_size, replace=False)\n",
    "    else:\n",
    "        indices = np.arange(len(X_test_text))\n",
    "\n",
    "    # Show selected indices, labels and a short preview of the selected samples\n",
    "    print(f\"Selected indices: {indices}\")\n",
    "    try:\n",
    "        print(f\"Selected labels (0=HAM,1=SPAM): {y_test_bin[indices]}\")\n",
    "    except Exception:\n",
    "        # Fallback if y_test_bin is a pandas Series\n",
    "        print(f\"Selected labels (0=HAM,1=SPAM): {np.asarray(y_test_bin)[indices]}\")\n",
    "    print(\"Selected sample texts (first 3 shown):\")\n",
    "    for idx in indices[:3]:\n",
    "        txt = X_test_text.iloc[idx] if hasattr(X_test_text, 'iloc') else X_test_text[idx]\n",
    "        print(f\" - idx {idx}: {str(txt)[:200].replace('\\n', ' ')}\")\n",
    "\n",
    "    # Run Ollama on the explicit indices (pass indices so the function doesn't resample)\n",
    "    preds, raw_outputs, indices = run_ollama_model(\"gemma\", X_test_text.values, sample_size=None, timeout=600, verbose=True, indices=indices)\n",
    "\n",
    "    # Align sample indices to label vector (y_test_bin defined earlier)\n",
    "    y_test_sample = np.asarray(y_test_bin)[indices]\n",
    "    ollama_predictions = preds\n",
    "\n",
    "    # Compute metrics for Gemma\n",
    "    accuracy_gemma = accuracy_score(y_test_sample, ollama_predictions)\n",
    "    precision_gemma = precision_score(y_test_sample, ollama_predictions, average='macro', zero_division=0)\n",
    "    recall_gemma = recall_score(y_test_sample, ollama_predictions, average='macro', zero_division=0)\n",
    "    f1_gemma = f1_score(y_test_sample, ollama_predictions, average='macro', zero_division=0)\n",
    "\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"GEMMA (Ollama) - RESULTS (on sampled test set)\")\n",
    "    print(\"=\"*70)\n",
    "    print(f\"Accuracy: {accuracy_gemma:.4f}\")\n",
    "    print(f\"Precision (macro): {precision_gemma:.4f}\")\n",
    "    print(f\"Recall (macro): {recall_gemma:.4f}\")\n",
    "    print(f\"F1-Score (macro): {f1_gemma:.4f}\")\n",
    "    print(f\"\\n✓ Sample predictions: {['SPAM' if p==1 else 'HAM' for p in ollama_predictions[:20]]} (showing up to 20)\")\n",
    "\n",
    "    print(f\"\\nNote: Full evaluation skipped. Ollama inference is time-intensive.\")\n",
    "else:\n",
    "    print(\"\\nSkipping Ollama (Gemma) evaluation until service is available.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23637f8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking if Ollama is available...\n",
      "\n",
      "✓ Ollama is available\n",
      "\n",
      "Available models:\n",
      "NAME            ID              SIZE      MODIFIED   \n",
      "gpt-oss:20b     17052f91a42e    13 GB     7 days ago    \n",
      "gemma:latest    a72c7f4d0a15    5.0 GB    8 days ago    \n",
      "\n",
      "\n",
      "Running Ollama (GPT) inference on sampled test set...\n",
      "\n",
      "Selected indices: [1832]\n",
      "Selected labels (0=HAM,1=SPAM): [1]\n",
      "Selected sample text:\n",
      " - idx 1832: transfers from ees attached is the latest version of the cost center assignments for the transfers out of ees . these transfers will be effective july 1 , 2001 and i need to get this to hr by friday , june 1 , 2001 to give them time to get everything effected . i think i have incorporated all your comments , but please review one more time and make sure we have not included anyone we shouldn ' t have or excluded anyone . you ' ll note that at this point we are not forming east and west risk management cost centers . don and rogers have decided for cost management purposes to leave it consolidated at this point . once you have signed off on your groupings , rachel massey in corporate planning will be working with each of you to forecast your q 3 and q 4 cost center expense plans . please let me know asap of any changes and don ' t hesitate to call with questions . thanks wade\n",
      "  Running gpt-oss:20b: 0/1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Python(5567) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "Python(5568) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "GPT (Ollama) - RESULTS (on sampled test set)\n",
      "======================================================================\n",
      "Accuracy: 0.0000\n",
      "Precision (macro): 0.0000\n",
      "Recall (macro): 0.0000\n",
      "F1-Score (macro): 0.0000\n",
      "\n",
      "✓ Sample predictions: ['HAM'] (showing up to 20)\n",
      "\n",
      "Note: Full evaluation skipped. Ollama inference is time-intensive.\n"
     ]
    }
   ],
   "source": [
    "import subprocess\n",
    "\n",
    "print(\"Checking if Ollama is available...\\n\")\n",
    "\n",
    "# Test if Ollama is running\n",
    "try:\n",
    "    result = subprocess.run(['ollama', 'list'], capture_output=True, timeout=5, text=True)\n",
    "    if result.returncode == 0:\n",
    "        print(\"✓ Ollama is available\")\n",
    "        ollama_available = True\n",
    "        print(f\"\\nAvailable models:\\n{result.stdout}\")\n",
    "    else:\n",
    "        print(\"⚠ Ollama not accessible. Installation required.\")\n",
    "        ollama_available = False\n",
    "except:\n",
    "    print(\"⚠ Ollama not installed or not running\")\n",
    "    print(\"\\nTo use Ollama:\")\n",
    "    print(\"  1. Install from https://ollama.ai\")\n",
    "    print(\"  2. Run: ollama pull gemma\")\n",
    "    print(\"  3. Start Ollama service\")\n",
    "    ollama_available = False\n",
    "\n",
    "if ollama_available:\n",
    "    print(\"\\nRunning Ollama (GPT) inference on sampled test set...\\n\")\n",
    "    # Sample a modest-sized subset for evaluation (adjustable)\n",
    "    sample_size = min(20, len(X_test_text))\n",
    "\n",
    "    # Choose indices explicitly so we can inspect the sample before running\n",
    "    if sample_size < len(X_test_text):\n",
    "        indices = np.random.choice(len(X_test_text), sample_size, replace=False)\n",
    "    else:\n",
    "        indices = np.arange(len(X_test_text))\n",
    "\n",
    "    # Print selected label and the sample text before running the model\n",
    "    print(f\"Selected indices: {indices}\")\n",
    "    try:\n",
    "        print(f\"Selected labels (0=HAM,1=SPAM): {y_test_bin[indices]}\")\n",
    "    except Exception:\n",
    "        print(f\"Selected labels (0=HAM,1=SPAM): {np.asarray(y_test_bin)[indices]}\")\n",
    "    print(\"Selected sample text:\")\n",
    "    for idx in indices:\n",
    "        txt = X_test_text.iloc[idx] if hasattr(X_test_text, 'iloc') else X_test_text[idx]\n",
    "        print(f\" - idx {idx}: {str(txt)[:1000].replace('\\n',' ')}\")\n",
    "\n",
    "    preds, raw_outputs, indices = run_ollama_model(\"gpt-oss:20b\", X_test_text.values, sample_size=None, timeout=600, verbose=True, indices=indices)\n",
    "    \n",
    "    # Align sample indices to label vector (y_test_bin defined earlier)\n",
    "    y_test_sample = np.asarray(y_test_bin)[indices]\n",
    "    ollama_predictions = preds\n",
    "\n",
    "    # Compute metrics for GPT model\n",
    "    accuracy_gpt = accuracy_score(y_test_sample, ollama_predictions)\n",
    "    precision_gpt = precision_score(y_test_sample, ollama_predictions, average='macro', zero_division=0)\n",
    "    recall_gpt = recall_score(y_test_sample, ollama_predictions, average='macro', zero_division=0)\n",
    "    f1_gpt = f1_score(y_test_sample, ollama_predictions, average='macro', zero_division=0)\n",
    "\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"GPT (Ollama) - RESULTS (on sampled test set)\")\n",
    "    print(\"=\"*70)\n",
    "    print(f\"Accuracy: {accuracy_gpt:.4f}\")\n",
    "    print(f\"Precision (macro): {precision_gpt:.4f}\")\n",
    "    print(f\"Recall (macro): {recall_gpt:.4f}\")\n",
    "    print(f\"F1-Score (macro): {f1_gpt:.4f}\")\n",
    "    print(f\"\\n✓ Sample predictions: {['SPAM' if p==1 else 'HAM' for p in ollama_predictions[:20]]} (showing up to 20)\")\n",
    "\n",
    "    print(f\"\\nNote: Full evaluation skipped. Ollama inference is time-intensive.\")\n",
    "else:\n",
    "    print(\"\\nSkipping Ollama (GPT) evaluation until service is available.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ac01008",
   "metadata": {},
   "source": [
    "## 11. Comprehensive Results Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "f802f8be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====================================================================================================\n",
      "COMPREHENSIVE MODEL RESULTS COMPARISON\n",
      "====================================================================================================\n",
      "\n",
      "              Model               Feature  Accuracy  Precision   Recall  F1-Score\n",
      "Logistic Regression               UNIGRAM  0.999406   0.999418 0.999394  0.999406\n",
      "Logistic Regression                BIGRAM  0.999406   0.999418 0.999394  0.999406\n",
      "Logistic Regression                   MIX  0.999703   0.999709 0.999697  0.999703\n",
      "Logistic Regression                 TFIDF  0.999109   0.999128 0.999091  0.999109\n",
      "        Naive Bayes               UNIGRAM  0.985447   0.986127 0.985152  0.985430\n",
      "        Naive Bayes                BIGRAM  0.983665   0.984481 0.983333  0.983644\n",
      "        Naive Bayes                   MIX  0.986041   0.986678 0.985758  0.986025\n",
      "        Naive Bayes                 TFIDF  0.987229   0.987784 0.986970  0.987215\n",
      "               LSTM    Sequence Embedding  0.997624   0.997681 0.997576  0.997623\n",
      "               BERT Transformer (Sampled)  0.510000   0.255000 0.500000  0.337748\n",
      "     Gemma (Ollama)       Gemma (Sampled)  0.700000   0.812500 0.700000  0.670330\n",
      "       GPT (Ollama)         GPT (Sampled)  0.000000   0.000000 0.000000  0.000000\n",
      "\n",
      "====================================================================================================\n",
      "TOP PERFORMING MODELS\n",
      "====================================================================================================\n",
      "\n",
      "Best Accuracy: Logistic Regression (MIX)\n",
      "  → Accuracy: 0.9997\n",
      "\n",
      "Best F1-Score (macro): Logistic Regression (MIX)\n",
      "  → F1-Score: 0.9997\n",
      "  → Accuracy: 0.9997\n",
      "\n",
      "====================================================================================================\n",
      "FEATURE ENGINEERING ANALYSIS\n",
      "====================================================================================================\n",
      "\n",
      "UNIGRAM:\n",
      "  Avg Accuracy: 0.9924\n",
      "  Avg F1-Score: 0.9924\n",
      "\n",
      "BIGRAM:\n",
      "  Avg Accuracy: 0.9915\n",
      "  Avg F1-Score: 0.9915\n",
      "\n",
      "MIX:\n",
      "  Avg Accuracy: 0.9929\n",
      "  Avg F1-Score: 0.9929\n",
      "\n",
      "TFIDF:\n",
      "  Avg Accuracy: 0.9932\n",
      "  Avg F1-Score: 0.9932\n",
      "\n",
      "====================================================================================================\n"
     ]
    }
   ],
   "source": [
    "# Compile all results\n",
    "results_data = []\n",
    "\n",
    "# Logistic Regression results\n",
    "for feature_name, metrics in lr_results.items():\n",
    "    results_data.append({\n",
    "        'Model': 'Logistic Regression',\n",
    "        'Feature': feature_name.upper(),\n",
    "        'Accuracy': metrics['accuracy'],\n",
    "        'Precision': metrics['precision'],\n",
    "        'Recall': metrics['recall'],\n",
    "        'F1-Score': metrics['f1']\n",
    "    })\n",
    "\n",
    "# Naive Bayes results\n",
    "for feature_name, metrics in nb_results.items():\n",
    "    results_data.append({\n",
    "        'Model': 'Naive Bayes',\n",
    "        'Feature': feature_name.upper(),\n",
    "        'Accuracy': metrics['accuracy'],\n",
    "        'Precision': metrics['precision'],\n",
    "        'Recall': metrics['recall'],\n",
    "        'F1-Score': metrics['f1']\n",
    "    })\n",
    "\n",
    "# LSTM results\n",
    "results_data.append({\n",
    "    'Model': 'LSTM',\n",
    "    'Feature': 'Sequence Embedding',\n",
    "    'Accuracy': accuracy_lstm,\n",
    "    'Precision': precision_lstm,\n",
    "    'Recall': recall_lstm,\n",
    "    'F1-Score': f1_lstm\n",
    "})\n",
    "\n",
    "# BERT results\n",
    "results_data.append({\n",
    "    'Model': 'BERT',\n",
    "    'Feature': 'Transformer (Sampled)',\n",
    "    'Accuracy': accuracy_bert,\n",
    "    'Precision': precision_bert,\n",
    "    'Recall': recall_bert,\n",
    "    'F1-Score': f1_bert\n",
    "})\n",
    "\n",
    "# Add Gemma (Ollama) if metrics were computed\n",
    "if 'accuracy_gemma' in globals():\n",
    "    results_data.append({\n",
    "        'Model': 'Gemma (Ollama)',\n",
    "        'Feature': 'Gemma (Sampled)',\n",
    "        'Accuracy': accuracy_gemma,\n",
    "        'Precision': precision_gemma,\n",
    "        'Recall': recall_gemma,\n",
    "        'F1-Score': f1_gemma\n",
    "    })\n",
    "else:\n",
    "    print('Gemma metrics not available; skipping')\n",
    "\n",
    "# Add GPT (Ollama) if metrics were computed\n",
    "if 'accuracy_gpt' in globals():\n",
    "    results_data.append({\n",
    "        'Model': 'GPT (Ollama)',\n",
    "        'Feature': 'GPT (Sampled)',\n",
    "        'Accuracy': accuracy_gpt,\n",
    "        'Precision': precision_gpt,\n",
    "        'Recall': recall_gpt,\n",
    "        'F1-Score': f1_gpt\n",
    "    })\n",
    "else:\n",
    "    print('GPT metrics not available; skipping')\n",
    "\n",
    "# Create results dataframe\n",
    "results_df = pd.DataFrame(results_data)\n",
    "\n",
    "print(\"=\"*100)\n",
    "print(\"COMPREHENSIVE MODEL RESULTS COMPARISON\")\n",
    "print(\"=\"*100)\n",
    "print(f\"\\n{results_df.to_string(index=False)}\")\n",
    "\n",
    "# Best models\n",
    "print(\"\\n\" + \"=\"*100)\n",
    "print(\"TOP PERFORMING MODELS\")\n",
    "print(\"=\"*100)\n",
    "\n",
    "if results_df.empty:\n",
    "    print('\\nNo results to summarize.')\n",
    "else:\n",
    "    best_accuracy_idx = results_df['Accuracy'].idxmax()\n",
    "    best_f1_idx = results_df['F1-Score'].idxmax()\n",
    "\n",
    "    best_accuracy = results_df.loc[best_accuracy_idx]\n",
    "    best_f1 = results_df.loc[best_f1_idx]\n",
    "\n",
    "    print(f\"\\nBest Accuracy: {best_accuracy['Model']} ({best_accuracy['Feature']})\")\n",
    "    print(f\"  → Accuracy: {best_accuracy['Accuracy']:.4f}\")\n",
    "\n",
    "    print(f\"\\nBest F1-Score (macro): {best_f1['Model']} ({best_f1['Feature']})\")\n",
    "    print(f\"  → F1-Score: {best_f1['F1-Score']:.4f}\")\n",
    "    print(f\"  → Accuracy: {best_f1['Accuracy']:.4f}\")\n",
    "\n",
    "# Feature engineering comparison\n",
    "print(\"\\n\" + \"=\"*100)\n",
    "print(\"FEATURE ENGINEERING ANALYSIS\")\n",
    "print(\"=\"*100)\n",
    "\n",
    "for feature in ['UNIGRAM', 'BIGRAM', 'MIX', 'TFIDF']:\n",
    "    feature_results = results_df[results_df['Feature'] == feature]\n",
    "    if len(feature_results) > 0:\n",
    "        avg_accuracy = feature_results['Accuracy'].mean()\n",
    "        avg_f1 = feature_results['F1-Score'].mean()\n",
    "        print(f\"\\n{feature}:\")\n",
    "        print(f\"  Avg Accuracy: {avg_accuracy:.4f}\")\n",
    "        print(f\"  Avg F1-Score: {avg_f1:.4f}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*100)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
