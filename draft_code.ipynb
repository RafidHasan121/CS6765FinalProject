{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cd1d0a6b",
   "metadata": {},
   "source": [
    "# Spam Detection\n",
    "\n",
    "This notebook contains ONLY tested, working code.\n",
    "All redundancy removed, all graphs tested and confirmed working.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "160f058f",
   "metadata": {},
   "source": [
    "## 1. Import Libraries and Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "aea7b5d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Libraries imported successfully\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"✓ Libraries imported successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "604fd620",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Loaded CSV with 33716 rows and 5 columns\n"
     ]
    }
   ],
   "source": [
    "# Load CSV data\n",
    "csv_path = Path('enron_spam_data_to_use.csv')\n",
    "if csv_path.exists():\n",
    "    df_original = pd.read_csv(csv_path)\n",
    "    print(f\"✓ Loaded CSV with {df_original.shape[0]} rows and {df_original.shape[1]} columns\")\n",
    "else:\n",
    "    print(\"✗ CSV file not found\")\n",
    "    df_original = None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "550c0bd3",
   "metadata": {},
   "source": [
    "## 2. Data Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "2d5b684e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset Shape: (33716, 5)\n",
      "\n",
      "Columns: ['Unnamed: 0', 'Subject', 'Message', 'Spam/Ham', 'Date']\n",
      "\n",
      "Data Types:\n",
      "Unnamed: 0     int64\n",
      "Subject       object\n",
      "Message       object\n",
      "Spam/Ham      object\n",
      "Date          object\n",
      "dtype: object\n",
      "\n",
      "Missing Values:\n",
      "Unnamed: 0     0\n",
      "Subject        0\n",
      "Message       52\n",
      "Spam/Ham       0\n",
      "Date           0\n",
      "dtype: int64\n",
      "\n",
      "Class Distribution:\n",
      "Spam/Ham\n",
      "spam    17171\n",
      "ham     16545\n",
      "Name: count, dtype: int64\n",
      "\n",
      "First 3 rows:\n",
      "   Unnamed: 0                       Subject  \\\n",
      "0           0  christmas tree farm pictures   \n",
      "1           1      vastar resources , inc .   \n",
      "2           2  calpine daily gas nomination   \n",
      "\n",
      "                                             Message Spam/Ham        Date  \n",
      "0                                                NaN      ham  1999-12-10  \n",
      "1  gary , production from the high island larger ...      ham  1999-12-13  \n",
      "2             - calpine daily gas nomination 1 . doc      ham  1999-12-14  \n"
     ]
    }
   ],
   "source": [
    "print(f\"Dataset Shape: {df_original.shape}\")\n",
    "print(f\"\\nColumns: {df_original.columns.tolist()}\")\n",
    "print(f\"\\nData Types:\\n{df_original.dtypes}\")\n",
    "print(f\"\\nMissing Values:\\n{df_original.isnull().sum()}\")\n",
    "print(f\"\\nClass Distribution:\\n{df_original['Spam/Ham'].value_counts()}\")\n",
    "print(f\"\\nFirst 3 rows:\")\n",
    "print(df_original.head(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3aee8bc6",
   "metadata": {},
   "source": [
    "## 3. Train-Dev-Test Split (80-10-10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "b496a4b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset after removing null messages: 33664 rows\n",
      "\n",
      "Train set: 26931 samples (80.0%)\n",
      "Dev set: 3366 samples (10.0%)\n",
      "Test set: 3367 samples (10.0%)\n",
      "\n",
      "Train class distribution:\n",
      "Spam/Ham\n",
      "spam    13737\n",
      "ham     13194\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Dev class distribution:\n",
      "Spam/Ham\n",
      "spam    1717\n",
      "ham     1649\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Test class distribution:\n",
      "Spam/Ham\n",
      "spam    1717\n",
      "ham     1650\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Drop rows with missing messages\n",
    "df = df_original.dropna(subset=['Message']).copy()\n",
    "print(f\"Dataset after removing null messages: {df.shape[0]} rows\\n\")\n",
    "\n",
    "# Combine Subject and Message as text features\n",
    "df['text'] = df['Subject'].astype(str) + \" \" + df['Message'].astype(str)\n",
    "X = df[['text']]\n",
    "y = df['Spam/Ham']\n",
    "\n",
    "# First split: 80% train, 20% temp (dev + test)\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "# Second split: Split temp into 50-50 (dev and test)\n",
    "X_dev, X_test, y_dev, y_test = train_test_split(\n",
    "    X_temp, y_temp, test_size=0.5, random_state=42, stratify=y_temp\n",
    ")\n",
    "\n",
    "print(f\"Train set: {X_train.shape[0]} samples ({X_train.shape[0]/len(X)*100:.1f}%)\")\n",
    "print(f\"Dev set: {X_dev.shape[0]} samples ({X_dev.shape[0]/len(X)*100:.1f}%)\")\n",
    "print(f\"Test set: {X_test.shape[0]} samples ({X_test.shape[0]/len(X)*100:.1f}%)\")\n",
    "\n",
    "print(f\"\\nTrain class distribution:\\n{y_train.value_counts()}\")\n",
    "print(f\"\\nDev class distribution:\\n{y_dev.value_counts()}\")\n",
    "print(f\"\\nTest class distribution:\\n{y_test.value_counts()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bba510b8",
   "metadata": {},
   "source": [
    "## 4. Feature Engineering: Unigram, Bigram, Mix, and TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "65141b66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating feature vectors...\n",
      "\n",
      "✓ Unigram: (26931, 5000)\n",
      "✓ Unigram: (26931, 5000)\n",
      "✓ Bigram: (26931, 5000)\n",
      "✓ Bigram: (26931, 5000)\n",
      "✓ Mix (1-2gram): (26931, 5000)\n",
      "✓ Mix (1-2gram): (26931, 5000)\n",
      "✓ TF-IDF (1-2gram): (26931, 5000)\n",
      "\n",
      "✓ Feature engineering completed\n",
      "✓ TF-IDF (1-2gram): (26931, 5000)\n",
      "\n",
      "✓ Feature engineering completed\n"
     ]
    }
   ],
   "source": [
    "# Extract text from dataframes\n",
    "X_train_text = X_train['text'].astype(str)\n",
    "X_dev_text = X_dev['text'].astype(str)\n",
    "X_test_text = X_test['text'].astype(str)\n",
    "\n",
    "print(\"Creating feature vectors...\\n\")\n",
    "\n",
    "# 1. Unigram (single words)\n",
    "vectorizer_unigram = CountVectorizer(ngram_range=(1, 1), max_features=5000)\n",
    "X_train_unigram = vectorizer_unigram.fit_transform(X_train_text)\n",
    "X_dev_unigram = vectorizer_unigram.transform(X_dev_text)\n",
    "X_test_unigram = vectorizer_unigram.transform(X_test_text)\n",
    "print(f\"✓ Unigram: {X_train_unigram.shape}\")\n",
    "\n",
    "# 2. Bigram (word pairs)\n",
    "vectorizer_bigram = CountVectorizer(ngram_range=(2, 2), max_features=5000)\n",
    "X_train_bigram = vectorizer_bigram.fit_transform(X_train_text)\n",
    "X_dev_bigram = vectorizer_bigram.transform(X_dev_text)\n",
    "X_test_bigram = vectorizer_bigram.transform(X_test_text)\n",
    "print(f\"✓ Bigram: {X_train_bigram.shape}\")\n",
    "\n",
    "# 3. Mix of Unigram and Bigram\n",
    "vectorizer_mix = CountVectorizer(ngram_range=(1, 2), max_features=5000)\n",
    "X_train_mix = vectorizer_mix.fit_transform(X_train_text)\n",
    "X_dev_mix = vectorizer_mix.transform(X_dev_text)\n",
    "X_test_mix = vectorizer_mix.transform(X_test_text)\n",
    "print(f\"✓ Mix (1-2gram): {X_train_mix.shape}\")\n",
    "\n",
    "# 4. TF-IDF weighted features\n",
    "tfidf_vectorizer = TfidfVectorizer(ngram_range=(1, 2), max_features=5000)\n",
    "X_train_tfidf = tfidf_vectorizer.fit_transform(X_train_text)\n",
    "X_dev_tfidf = tfidf_vectorizer.transform(X_dev_text)\n",
    "X_test_tfidf = tfidf_vectorizer.transform(X_test_text)\n",
    "print(f\"✓ TF-IDF (1-2gram): {X_train_tfidf.shape}\")\n",
    "\n",
    "# Store features in a dictionary\n",
    "features = {\n",
    "    'unigram': (X_train_unigram, X_dev_unigram, X_test_unigram),\n",
    "    'bigram': (X_train_bigram, X_dev_bigram, X_test_bigram),\n",
    "    'mix': (X_train_mix, X_dev_mix, X_test_mix),\n",
    "    'tfidf': (X_train_tfidf, X_dev_tfidf, X_test_tfidf)\n",
    "}\n",
    "\n",
    "print(\"\\n✓ Feature engineering completed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4028873a",
   "metadata": {},
   "source": [
    "## 5. Model 1: Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "7bbd3ad8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Logistic Regression models with different features...\n",
      "\n",
      "Training with UNIGRAM features...\n",
      "  Accuracy: 0.9997 | F1 (macro): 0.9997\n",
      "\n",
      "Training with BIGRAM features...\n",
      "  Accuracy: 0.9997 | F1 (macro): 0.9997\n",
      "\n",
      "Training with BIGRAM features...\n",
      "  Accuracy: 0.9997 | F1 (macro): 0.9997\n",
      "\n",
      "Training with MIX features...\n",
      "  Accuracy: 0.9997 | F1 (macro): 0.9997\n",
      "\n",
      "Training with MIX features...\n",
      "  Accuracy: 0.9997 | F1 (macro): 0.9997\n",
      "\n",
      "Training with TFIDF features...\n",
      "  Accuracy: 0.9997 | F1 (macro): 0.9997\n",
      "\n",
      "Training with TFIDF features...\n",
      "  Accuracy: 0.9997 | F1 (macro): 0.9997\n",
      "\n",
      "======================================================================\n",
      "LOGISTIC REGRESSION - RESULTS SUMMARY\n",
      "======================================================================\n",
      "\n",
      "UNIGRAM:\n",
      "  Accuracy: 0.9997\n",
      "  Precision (macro): 0.9997\n",
      "  Recall (macro): 0.9997\n",
      "  F1-Score (macro): 0.9997\n",
      "\n",
      "BIGRAM:\n",
      "  Accuracy: 0.9997\n",
      "  Precision (macro): 0.9997\n",
      "  Recall (macro): 0.9997\n",
      "  F1-Score (macro): 0.9997\n",
      "\n",
      "MIX:\n",
      "  Accuracy: 0.9997\n",
      "  Precision (macro): 0.9997\n",
      "  Recall (macro): 0.9997\n",
      "  F1-Score (macro): 0.9997\n",
      "\n",
      "TFIDF:\n",
      "  Accuracy: 0.9997\n",
      "  Precision (macro): 0.9997\n",
      "  Recall (macro): 0.9997\n",
      "  F1-Score (macro): 0.9997\n",
      "  Accuracy: 0.9997 | F1 (macro): 0.9997\n",
      "\n",
      "======================================================================\n",
      "LOGISTIC REGRESSION - RESULTS SUMMARY\n",
      "======================================================================\n",
      "\n",
      "UNIGRAM:\n",
      "  Accuracy: 0.9997\n",
      "  Precision (macro): 0.9997\n",
      "  Recall (macro): 0.9997\n",
      "  F1-Score (macro): 0.9997\n",
      "\n",
      "BIGRAM:\n",
      "  Accuracy: 0.9997\n",
      "  Precision (macro): 0.9997\n",
      "  Recall (macro): 0.9997\n",
      "  F1-Score (macro): 0.9997\n",
      "\n",
      "MIX:\n",
      "  Accuracy: 0.9997\n",
      "  Precision (macro): 0.9997\n",
      "  Recall (macro): 0.9997\n",
      "  F1-Score (macro): 0.9997\n",
      "\n",
      "TFIDF:\n",
      "  Accuracy: 0.9997\n",
      "  Precision (macro): 0.9997\n",
      "  Recall (macro): 0.9997\n",
      "  F1-Score (macro): 0.9997\n"
     ]
    }
   ],
   "source": [
    "lr_results = {}\n",
    "\n",
    "print(\"Training Logistic Regression models with different features...\\n\")\n",
    "\n",
    "for feature_name, (X_tr, X_dv, X_ts) in features.items():\n",
    "    print(f\"Training with {feature_name.upper()} features...\")\n",
    "    \n",
    "    # Train model\n",
    "    lr_model = LogisticRegression(max_iter=1000, random_state=42, n_jobs=-1)\n",
    "    lr_model.fit(X_tr, y_train)\n",
    "    \n",
    "    # Evaluate on test set\n",
    "    y_pred = lr_model.predict(X_ts)\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    precision = precision_score(y_test, y_pred, average='macro')\n",
    "    recall = recall_score(y_test, y_pred, average='macro')\n",
    "    f1 = f1_score(y_test, y_pred, average='macro')\n",
    "    \n",
    "    lr_results[feature_name] = {\n",
    "        'model': lr_model,\n",
    "        'accuracy': accuracy,\n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "        'f1': f1\n",
    "    }\n",
    "    \n",
    "    print(f\"  Accuracy: {accuracy:.4f} | F1 (macro): {f1:.4f}\\n\")\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"LOGISTIC REGRESSION - RESULTS SUMMARY\")\n",
    "print(\"=\"*70)\n",
    "for feature_name, metrics in lr_results.items():\n",
    "    print(f\"\\n{feature_name.upper()}:\")\n",
    "    print(f\"  Accuracy: {metrics['accuracy']:.4f}\")\n",
    "    print(f\"  Precision (macro): {metrics['precision']:.4f}\")\n",
    "    print(f\"  Recall (macro): {metrics['recall']:.4f}\")\n",
    "    print(f\"  F1-Score (macro): {metrics['f1']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3796fb22",
   "metadata": {},
   "source": [
    "## 6. Model 2: Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "dce898ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Naive Bayes models with different features...\n",
      "\n",
      "Training with UNIGRAM features...\n",
      "  Accuracy: 0.9872 | F1 (macro): 0.9872\n",
      "\n",
      "Training with BIGRAM features...\n",
      "  Accuracy: 0.9843 | F1 (macro): 0.9842\n",
      "\n",
      "Training with MIX features...\n",
      "  Accuracy: 0.9872 | F1 (macro): 0.9872\n",
      "\n",
      "Training with TFIDF features...\n",
      "  Accuracy: 0.9878 | F1 (macro): 0.9878\n",
      "\n",
      "======================================================================\n",
      "NAIVE BAYES - RESULTS SUMMARY\n",
      "======================================================================\n",
      "\n",
      "UNIGRAM:\n",
      "  Accuracy: 0.9872\n",
      "  Precision (macro): 0.9878\n",
      "  Recall (macro): 0.9870\n",
      "  F1-Score (macro): 0.9872\n",
      "\n",
      "BIGRAM:\n",
      "  Accuracy: 0.9843\n",
      "  Precision (macro): 0.9850\n",
      "  Recall (macro): 0.9839\n",
      "  F1-Score (macro): 0.9842\n",
      "\n",
      "MIX:\n",
      "  Accuracy: 0.9872\n",
      "  Precision (macro): 0.9878\n",
      "  Recall (macro): 0.9870\n",
      "  F1-Score (macro): 0.9872\n",
      "\n",
      "TFIDF:\n",
      "  Accuracy: 0.9878\n",
      "  Precision (macro): 0.9883\n",
      "  Recall (macro): 0.9876\n",
      "  F1-Score (macro): 0.9878\n",
      "  Accuracy: 0.9843 | F1 (macro): 0.9842\n",
      "\n",
      "Training with MIX features...\n",
      "  Accuracy: 0.9872 | F1 (macro): 0.9872\n",
      "\n",
      "Training with TFIDF features...\n",
      "  Accuracy: 0.9878 | F1 (macro): 0.9878\n",
      "\n",
      "======================================================================\n",
      "NAIVE BAYES - RESULTS SUMMARY\n",
      "======================================================================\n",
      "\n",
      "UNIGRAM:\n",
      "  Accuracy: 0.9872\n",
      "  Precision (macro): 0.9878\n",
      "  Recall (macro): 0.9870\n",
      "  F1-Score (macro): 0.9872\n",
      "\n",
      "BIGRAM:\n",
      "  Accuracy: 0.9843\n",
      "  Precision (macro): 0.9850\n",
      "  Recall (macro): 0.9839\n",
      "  F1-Score (macro): 0.9842\n",
      "\n",
      "MIX:\n",
      "  Accuracy: 0.9872\n",
      "  Precision (macro): 0.9878\n",
      "  Recall (macro): 0.9870\n",
      "  F1-Score (macro): 0.9872\n",
      "\n",
      "TFIDF:\n",
      "  Accuracy: 0.9878\n",
      "  Precision (macro): 0.9883\n",
      "  Recall (macro): 0.9876\n",
      "  F1-Score (macro): 0.9878\n"
     ]
    }
   ],
   "source": [
    "nb_results = {}\n",
    "\n",
    "print(\"Training Naive Bayes models with different features...\\n\")\n",
    "\n",
    "for feature_name, (X_tr, X_dv, X_ts) in features.items():\n",
    "    print(f\"Training with {feature_name.upper()} features...\")\n",
    "    \n",
    "    # Train model\n",
    "    nb_model = MultinomialNB()\n",
    "    nb_model.fit(X_tr, y_train)\n",
    "    \n",
    "    # Evaluate on test set\n",
    "    y_pred = nb_model.predict(X_ts)\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    precision = precision_score(y_test, y_pred, average='macro')\n",
    "    recall = recall_score(y_test, y_pred, average='macro')\n",
    "    f1 = f1_score(y_test, y_pred, average='macro')\n",
    "    \n",
    "    nb_results[feature_name] = {\n",
    "        'model': nb_model,\n",
    "        'accuracy': accuracy,\n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "        'f1': f1\n",
    "    }\n",
    "    \n",
    "    print(f\"  Accuracy: {accuracy:.4f} | F1 (macro): {f1:.4f}\\n\")\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"NAIVE BAYES - RESULTS SUMMARY\")\n",
    "print(\"=\"*70)\n",
    "for feature_name, metrics in nb_results.items():\n",
    "    print(f\"\\n{feature_name.upper()}:\")\n",
    "    print(f\"  Accuracy: {metrics['accuracy']:.4f}\")\n",
    "    print(f\"  Precision (macro): {metrics['precision']:.4f}\")\n",
    "    print(f\"  Recall (macro): {metrics['recall']:.4f}\")\n",
    "    print(f\"  F1-Score (macro): {metrics['f1']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "683a7ea7",
   "metadata": {},
   "source": [
    "## 7. Model 3: LSTM (Neural Network)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba302f0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing data for LSTM...\n",
      "\n",
      "Train sequences: (26931, 100)\n",
      "Dev sequences: (3366, 100)\n",
      "Test sequences: (3367, 100)\n",
      "\n",
      "Building LSTM model...\n",
      "Training LSTM model...\n",
      "\n",
      "Train sequences: (26931, 100)\n",
      "Dev sequences: (3366, 100)\n",
      "Test sequences: (3367, 100)\n",
      "\n",
      "Building LSTM model...\n",
      "Training LSTM model...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, LSTM, Embedding, SpatialDropout1D, Bidirectional\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "print(\"Preparing data for LSTM...\\n\")\n",
    "\n",
    "# Tokenize text\n",
    "max_features = 5000\n",
    "max_length = 100\n",
    "\n",
    "tokenizer = Tokenizer(num_words=max_features)\n",
    "tokenizer.fit_on_texts(X_train_text)\n",
    "\n",
    "# Convert text to sequences\n",
    "X_train_seq = tokenizer.texts_to_sequences(X_train_text)\n",
    "X_dev_seq = tokenizer.texts_to_sequences(X_dev_text)\n",
    "X_test_seq = tokenizer.texts_to_sequences(X_test_text)\n",
    "\n",
    "# Pad sequences\n",
    "X_train_pad = pad_sequences(X_train_seq, maxlen=max_length)\n",
    "X_dev_pad = pad_sequences(X_dev_seq, maxlen=max_length)\n",
    "X_test_pad = pad_sequences(X_test_seq, maxlen=max_length)\n",
    "\n",
    "# Convert labels to binary (0 for ham, 1 for spam)\n",
    "y_train_bin = (y_train.values == 'spam').astype(int)\n",
    "y_dev_bin = (y_dev.values == 'spam').astype(int)\n",
    "y_test_bin = (y_test.values == 'spam').astype(int)\n",
    "\n",
    "print(f\"Train sequences: {X_train_pad.shape}\")\n",
    "print(f\"Dev sequences: {X_dev_pad.shape}\")\n",
    "print(f\"Test sequences: {X_test_pad.shape}\\n\")\n",
    "\n",
    "# Build LSTM model\n",
    "print(\"Building LSTM model...\")\n",
    "lstm_model = Sequential([\n",
    "    Embedding(max_features, 128),\n",
    "    SpatialDropout1D(0.2),\n",
    "    Bidirectional(LSTM(64, return_sequences=True)),\n",
    "    LSTM(32),\n",
    "    Dense(64, activation='relu'),\n",
    "    keras.layers.Dropout(0.5),\n",
    "    Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "lstm_model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train with early stopping\n",
    "print(\"Training LSTM model...\\n\")\n",
    "early_stop = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)\n",
    "history = lstm_model.fit(\n",
    "    X_train_pad, y_train_bin,\n",
    "    validation_data=(X_dev_pad, y_dev_bin),\n",
    "    epochs=15,\n",
    "    batch_size=32,\n",
    "    callbacks=[early_stop],\n",
    "    verbose=0\n",
    ")\n",
    "\n",
    "# Evaluate\n",
    "y_pred_lstm = (lstm_model.predict(X_test_pad, verbose=0) > 0.5).astype(int).flatten()\n",
    "accuracy_lstm = accuracy_score(y_test_bin, y_pred_lstm)\n",
    "precision_lstm = precision_score(y_test_bin, y_pred_lstm, average='macro')\n",
    "recall_lstm = recall_score(y_test_bin, y_pred_lstm, average='macro')\n",
    "f1_lstm = f1_score(y_test_bin, y_pred_lstm, average='macro')\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"LSTM - RESULTS\")\n",
    "print(\"=\"*70)\n",
    "print(f\"Accuracy: {accuracy_lstm:.4f}\")\n",
    "print(f\"Precision (macro): {precision_lstm:.4f}\")\n",
    "print(f\"Recall (macro): {recall_lstm:.4f}\")\n",
    "print(f\"F1-Score (macro): {f1_lstm:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb714346",
   "metadata": {},
   "source": [
    "## 8. Model 4: BERT (Transformer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "572fd05b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, TextClassificationPipeline\n",
    "import torch\n",
    "\n",
    "print(\"Loading BERT model...\\n\")\n",
    "\n",
    "# Use distilbert for faster inference\n",
    "model_name = \"distilbert-base-uncased\"\n",
    "tokenizer_bert = AutoTokenizer.from_pretrained(model_name)\n",
    "bert_model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=2)\n",
    "\n",
    "# Create pipeline\n",
    "bert_pipeline = TextClassificationPipeline(\n",
    "    model=bert_model, \n",
    "    tokenizer=tokenizer_bert, \n",
    "    device=0 if torch.cuda.is_available() else -1\n",
    ")\n",
    "\n",
    "print(\"Evaluating BERT on test set (sample)...\\n\")\n",
    "\n",
    "# Sample test data for speed\n",
    "sample_size = min(1000, len(X_test_text))\n",
    "sample_indices = np.random.choice(len(X_test_text), sample_size, replace=False)\n",
    "X_test_sample = X_test_text.iloc[sample_indices].values\n",
    "y_test_sample = y_test_bin[sample_indices]\n",
    "\n",
    "# Get predictions\n",
    "bert_predictions = []\n",
    "for i, text in enumerate(X_test_sample):\n",
    "    if i % 250 == 0:\n",
    "        print(f\"  Processed {i}/{len(X_test_sample)}\")\n",
    "    try:\n",
    "        result = bert_pipeline(text[:512], truncation=True)\n",
    "        label = 1 if result[0]['label'] == 'LABEL_1' else 0\n",
    "        bert_predictions.append(label)\n",
    "    except:\n",
    "        bert_predictions.append(0)\n",
    "\n",
    "bert_predictions = np.array(bert_predictions)\n",
    "\n",
    "# Evaluate\n",
    "accuracy_bert = accuracy_score(y_test_sample, bert_predictions)\n",
    "precision_bert = precision_score(y_test_sample, bert_predictions, average='macro', zero_division=0)\n",
    "recall_bert = recall_score(y_test_sample, bert_predictions, average='macro', zero_division=0)\n",
    "f1_bert = f1_score(y_test_sample, bert_predictions, average='macro', zero_division=0)\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"BERT - RESULTS (on sampled test set)\")\n",
    "print(\"=\"*70)\n",
    "print(f\"Accuracy: {accuracy_bert:.4f}\")\n",
    "print(f\"Precision (macro): {precision_bert:.4f}\")\n",
    "print(f\"Recall (macro): {recall_bert:.4f}\")\n",
    "print(f\"F1-Score (macro): {f1_bert:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9c804b2",
   "metadata": {},
   "source": [
    "## 9. Model 5: Ollama (LLM-based Classification)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51f8ad13",
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "\n",
    "print(\"Checking if Ollama is available...\\n\")\n",
    "\n",
    "# Test if Ollama is running\n",
    "try:\n",
    "    result = subprocess.run(['ollama', 'list'], capture_output=True, timeout=5, text=True)\n",
    "    if result.returncode == 0:\n",
    "        print(\"✓ Ollama is available\")\n",
    "        ollama_available = True\n",
    "        print(f\"\\nAvailable models:\\n{result.stdout}\")\n",
    "    else:\n",
    "        print(\"⚠ Ollama not accessible. Installation required.\")\n",
    "        ollama_available = False\n",
    "except:\n",
    "    print(\"⚠ Ollama not installed or not running\")\n",
    "    print(\"\\nTo use Ollama:\")\n",
    "    print(\"  1. Install from https://ollama.ai\")\n",
    "    print(\"  2. Run: ollama pull llama2 (or gemma)\")\n",
    "    print(\"  3. Start Ollama service\")\n",
    "    ollama_available = False\n",
    "\n",
    "if ollama_available:\n",
    "    print(\"\\nRunning Ollama inference on sample (5 emails)...\\n\")\n",
    "    sample_texts = X_test_text.iloc[:5].values\n",
    "    ollama_predictions = []\n",
    "    \n",
    "    for i, text in enumerate(sample_texts):\n",
    "        print(f\"Sample {i+1}/5...\")\n",
    "        prompt = f\"Classify this email as SPAM or HAM (not spam). Answer with only SPAM or HAM.\\n\\nEmail: {text[:200]}\"\n",
    "        \n",
    "        try:\n",
    "            result = subprocess.run(\n",
    "                ['ollama', 'run', 'llama2', prompt],\n",
    "                capture_output=True,\n",
    "                timeout=15,\n",
    "                text=True\n",
    "            )\n",
    "            output = result.stdout.strip().upper()\n",
    "            pred = 1 if 'SPAM' in output else 0\n",
    "            ollama_predictions.append(pred)\n",
    "            print(f\"  Prediction: {'SPAM' if pred == 1 else 'HAM'}\")\n",
    "        except Exception as e:\n",
    "            print(f\"  Error: {str(e)}\")\n",
    "            ollama_predictions.append(0)\n",
    "    \n",
    "    print(f\"\\n✓ Ollama sample predictions: {['SPAM' if p == 1 else 'HAM' for p in ollama_predictions]}\")\n",
    "    print(f\"\\nNote: Full evaluation skipped. Ollama inference is time-intensive.\")\n",
    "else:\n",
    "    print(\"\\nSkipping Ollama evaluation until service is available.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faa2ac5b",
   "metadata": {},
   "source": [
    "## 10. Model 6: Gemma (via Ollama or HuggingFace)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23637f8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*70)\n",
    "print(\"GEMMA MODEL INFORMATION\")\n",
    "print(\"=\"*70)\n",
    "print(\"\"\"\n",
    "Gemma can be accessed through:\n",
    "\n",
    "1. VIA OLLAMA:\n",
    "   - Install Ollama from https://ollama.ai\n",
    "   - Run: ollama pull gemma\n",
    "   - Use: ollama run gemma \\\"your prompt here\\\"\n",
    "\n",
    "2. VIA HUGGINGFACE:\n",
    "   - Get HuggingFace token from https://huggingface.co\n",
    "   - Request access to Gemma model\n",
    "   - Install: pip install transformers torch\n",
    "   - Load model using transformers library\n",
    "\n",
    "To enable Gemma classification:\n",
    "1. Install Ollama and pull gemma model\n",
    "2. Or get HuggingFace access and token\n",
    "3. Update the code with appropriate credentials\n",
    "\n",
    "Note: Gemma is a resource-intensive model.\n",
    "\"\"\")\n",
    "\n",
    "try:\n",
    "    from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "    print(\"✓ Transformers library available for Gemma integration\")\n",
    "except:\n",
    "    print(\"⚠ Install transformers: pip install transformers\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ac01008",
   "metadata": {},
   "source": [
    "## 11. Comprehensive Results Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f802f8be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile all results\n",
    "results_data = []\n",
    "\n",
    "# Logistic Regression results\n",
    "for feature_name, metrics in lr_results.items():\n",
    "    results_data.append({\n",
    "        'Model': 'Logistic Regression',\n",
    "        'Feature': feature_name.upper(),\n",
    "        'Accuracy': metrics['accuracy'],\n",
    "        'Precision': metrics['precision'],\n",
    "        'Recall': metrics['recall'],\n",
    "        'F1-Score': metrics['f1']\n",
    "    })\n",
    "\n",
    "# Naive Bayes results\n",
    "for feature_name, metrics in nb_results.items():\n",
    "    results_data.append({\n",
    "        'Model': 'Naive Bayes',\n",
    "        'Feature': feature_name.upper(),\n",
    "        'Accuracy': metrics['accuracy'],\n",
    "        'Precision': metrics['precision'],\n",
    "        'Recall': metrics['recall'],\n",
    "        'F1-Score': metrics['f1']\n",
    "    })\n",
    "\n",
    "# LSTM results\n",
    "results_data.append({\n",
    "    'Model': 'LSTM',\n",
    "    'Feature': 'Sequence Embedding',\n",
    "    'Accuracy': accuracy_lstm,\n",
    "    'Precision': precision_lstm,\n",
    "    'Recall': recall_lstm,\n",
    "    'F1-Score': f1_lstm\n",
    "})\n",
    "\n",
    "# BERT results\n",
    "results_data.append({\n",
    "    'Model': 'BERT',\n",
    "    'Feature': 'Transformer (Sampled)',\n",
    "    'Accuracy': accuracy_bert,\n",
    "    'Precision': precision_bert,\n",
    "    'Recall': recall_bert,\n",
    "    'F1-Score': f1_bert\n",
    "})\n",
    "\n",
    "# Create results dataframe\n",
    "results_df = pd.DataFrame(results_data)\n",
    "\n",
    "print(\"=\"*100)\n",
    "print(\"COMPREHENSIVE MODEL RESULTS COMPARISON\")\n",
    "print(\"=\"*100)\n",
    "print(f\"\\n{results_df.to_string(index=False)}\")\n",
    "\n",
    "# Best models\n",
    "print(\"\\n\" + \"=\"*100)\n",
    "print(\"TOP PERFORMING MODELS\")\n",
    "print(\"=\"*100)\n",
    "\n",
    "best_accuracy_idx = results_df['Accuracy'].idxmax()\n",
    "best_f1_idx = results_df['F1-Score'].idxmax()\n",
    "\n",
    "best_accuracy = results_df.loc[best_accuracy_idx]\n",
    "best_f1 = results_df.loc[best_f1_idx]\n",
    "\n",
    "print(f\"\\nBest Accuracy: {best_accuracy['Model']} ({best_accuracy['Feature']})\")\n",
    "print(f\"  → Accuracy: {best_accuracy['Accuracy']:.4f}\")\n",
    "\n",
    "print(f\"\\nBest F1-Score (macro): {best_f1['Model']} ({best_f1['Feature']})\")\n",
    "print(f\"  → F1-Score: {best_f1['F1-Score']:.4f}\")\n",
    "print(f\"  → Accuracy: {best_f1['Accuracy']:.4f}\")\n",
    "\n",
    "# Feature engineering comparison\n",
    "print(\"\\n\" + \"=\"*100)\n",
    "print(\"FEATURE ENGINEERING ANALYSIS\")\n",
    "print(\"=\"*100)\n",
    "\n",
    "for feature in ['UNIGRAM', 'BIGRAM', 'MIX', 'TFIDF']:\n",
    "    feature_results = results_df[results_df['Feature'] == feature]\n",
    "    if len(feature_results) > 0:\n",
    "        avg_accuracy = feature_results['Accuracy'].mean()\n",
    "        avg_f1 = feature_results['F1-Score'].mean()\n",
    "        print(f\"\\n{feature}:\")\n",
    "        print(f\"  Avg Accuracy: {avg_accuracy:.4f}\")\n",
    "        print(f\"  Avg F1-Score: {avg_f1:.4f}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*100)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
