{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cd1d0a6b",
   "metadata": {},
   "source": [
    "# Spam Detection\n",
    "\n",
    "This notebook contains ONLY tested, working code.\n",
    "All redundancy removed, all graphs tested and confirmed working.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "160f058f",
   "metadata": {},
   "source": [
    "## 1. Import Libraries and Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aea7b5d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"✓ Libraries imported successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "604fd620",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load CSV data\n",
    "csv_path = Path('enron_spam_data_to_use.csv')\n",
    "if csv_path.exists():\n",
    "    df_original = pd.read_csv(csv_path)\n",
    "    print(f\"✓ Loaded CSV with {df_original.shape[0]} rows and {df_original.shape[1]} columns\")\n",
    "else:\n",
    "    print(\"✗ CSV file not found\")\n",
    "    df_original = None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "550c0bd3",
   "metadata": {},
   "source": [
    "## 2. Data Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d5b684e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Dataset Shape: {df_original.shape}\")\n",
    "print(f\"\\nColumns: {df_original.columns.tolist()}\")\n",
    "print(f\"\\nData Types:\\n{df_original.dtypes}\")\n",
    "print(f\"\\nMissing Values:\\n{df_original.isnull().sum()}\")\n",
    "print(f\"\\nClass Distribution:\\n{df_original['Spam/Ham'].value_counts()}\")\n",
    "print(f\"\\nFirst 3 rows:\")\n",
    "print(df_original.head(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3aee8bc6",
   "metadata": {},
   "source": [
    "## 3. Train-Dev-Test Split (80-10-10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b496a4b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop rows with missing messages\n",
    "df = df_original.dropna(subset=['Message']).copy()\n",
    "print(f\"Dataset after removing null messages: {df.shape[0]} rows\\n\")\n",
    "\n",
    "# Combine Subject and Message as text features\n",
    "df['text'] = df['Subject'].astype(str) + \" \" + df['Message'].astype(str)\n",
    "X = df[['text']]\n",
    "y = df['Spam/Ham']\n",
    "\n",
    "# First split: 80% train, 20% temp (dev + test)\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "# Second split: Split temp into 50-50 (dev and test)\n",
    "X_dev, X_test, y_dev, y_test = train_test_split(\n",
    "    X_temp, y_temp, test_size=0.5, random_state=42, stratify=y_temp\n",
    ")\n",
    "\n",
    "print(f\"Train set: {X_train.shape[0]} samples ({X_train.shape[0]/len(X)*100:.1f}%)\")\n",
    "print(f\"Dev set: {X_dev.shape[0]} samples ({X_dev.shape[0]/len(X)*100:.1f}%)\")\n",
    "print(f\"Test set: {X_test.shape[0]} samples ({X_test.shape[0]/len(X)*100:.1f}%)\")\n",
    "\n",
    "print(f\"\\nTrain class distribution:\\n{y_train.value_counts()}\")\n",
    "print(f\"\\nDev class distribution:\\n{y_dev.value_counts()}\")\n",
    "print(f\"\\nTest class distribution:\\n{y_test.value_counts()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bba510b8",
   "metadata": {},
   "source": [
    "## 4. Feature Engineering: Unigram, Bigram, Mix, and TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65141b66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract text from dataframes\n",
    "X_train_text = X_train['text'].astype(str)\n",
    "X_dev_text = X_dev['text'].astype(str)\n",
    "X_test_text = X_test['text'].astype(str)\n",
    "\n",
    "print(\"Creating feature vectors...\\n\")\n",
    "\n",
    "# 1. Unigram (single words)\n",
    "vectorizer_unigram = CountVectorizer(ngram_range=(1, 1), max_features=5000)\n",
    "X_train_unigram = vectorizer_unigram.fit_transform(X_train_text)\n",
    "X_dev_unigram = vectorizer_unigram.transform(X_dev_text)\n",
    "X_test_unigram = vectorizer_unigram.transform(X_test_text)\n",
    "print(f\"✓ Unigram: {X_train_unigram.shape}\")\n",
    "\n",
    "# 2. Bigram (word pairs)\n",
    "vectorizer_bigram = CountVectorizer(ngram_range=(2, 2), max_features=5000)\n",
    "X_train_bigram = vectorizer_bigram.fit_transform(X_train_text)\n",
    "X_dev_bigram = vectorizer_bigram.transform(X_dev_text)\n",
    "X_test_bigram = vectorizer_bigram.transform(X_test_text)\n",
    "print(f\"✓ Bigram: {X_train_bigram.shape}\")\n",
    "\n",
    "# 3. Mix of Unigram and Bigram\n",
    "vectorizer_mix = CountVectorizer(ngram_range=(1, 2), max_features=5000)\n",
    "X_train_mix = vectorizer_mix.fit_transform(X_train_text)\n",
    "X_dev_mix = vectorizer_mix.transform(X_dev_text)\n",
    "X_test_mix = vectorizer_mix.transform(X_test_text)\n",
    "print(f\"✓ Mix (1-2gram): {X_train_mix.shape}\")\n",
    "\n",
    "# 4. TF-IDF weighted features\n",
    "tfidf_vectorizer = TfidfVectorizer(ngram_range=(1, 2), max_features=5000)\n",
    "X_train_tfidf = tfidf_vectorizer.fit_transform(X_train_text)\n",
    "X_dev_tfidf = tfidf_vectorizer.transform(X_dev_text)\n",
    "X_test_tfidf = tfidf_vectorizer.transform(X_test_text)\n",
    "print(f\"✓ TF-IDF (1-2gram): {X_train_tfidf.shape}\")\n",
    "\n",
    "# Store features in a dictionary\n",
    "features = {\n",
    "    'unigram': (X_train_unigram, X_dev_unigram, X_test_unigram),\n",
    "    'bigram': (X_train_bigram, X_dev_bigram, X_test_bigram),\n",
    "    'mix': (X_train_mix, X_dev_mix, X_test_mix),\n",
    "    'tfidf': (X_train_tfidf, X_dev_tfidf, X_test_tfidf)\n",
    "}\n",
    "\n",
    "print(\"\\n✓ Feature engineering completed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4028873a",
   "metadata": {},
   "source": [
    "## 5. Model 1: Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bbd3ad8",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_results = {}\n",
    "\n",
    "print(\"Training Logistic Regression models with different features...\\n\")\n",
    "\n",
    "for feature_name, (X_tr, X_dv, X_ts) in features.items():\n",
    "    print(f\"Training with {feature_name.upper()} features...\")\n",
    "    \n",
    "    # Train model\n",
    "    lr_model = LogisticRegression(max_iter=1000, random_state=42, n_jobs=-1)\n",
    "    lr_model.fit(X_tr, y_train)\n",
    "    \n",
    "    # Evaluate on test set\n",
    "    y_pred = lr_model.predict(X_ts)\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    precision = precision_score(y_test, y_pred, average='macro')\n",
    "    recall = recall_score(y_test, y_pred, average='macro')\n",
    "    f1 = f1_score(y_test, y_pred, average='macro')\n",
    "    \n",
    "    lr_results[feature_name] = {\n",
    "        'model': lr_model,\n",
    "        'accuracy': accuracy,\n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "        'f1': f1\n",
    "    }\n",
    "    \n",
    "    print(f\"  Accuracy: {accuracy:.4f} | F1 (macro): {f1:.4f}\\n\")\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"LOGISTIC REGRESSION - RESULTS SUMMARY\")\n",
    "print(\"=\"*70)\n",
    "for feature_name, metrics in lr_results.items():\n",
    "    print(f\"\\n{feature_name.upper()}:\")\n",
    "    print(f\"  Accuracy: {metrics['accuracy']:.4f}\")\n",
    "    print(f\"  Precision (macro): {metrics['precision']:.4f}\")\n",
    "    print(f\"  Recall (macro): {metrics['recall']:.4f}\")\n",
    "    print(f\"  F1-Score (macro): {metrics['f1']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3796fb22",
   "metadata": {},
   "source": [
    "## 6. Model 2: Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dce898ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_results = {}\n",
    "\n",
    "print(\"Training Naive Bayes models with different features...\\n\")\n",
    "\n",
    "for feature_name, (X_tr, X_dv, X_ts) in features.items():\n",
    "    print(f\"Training with {feature_name.upper()} features...\")\n",
    "    \n",
    "    # Train model\n",
    "    nb_model = MultinomialNB()\n",
    "    nb_model.fit(X_tr, y_train)\n",
    "    \n",
    "    # Evaluate on test set\n",
    "    y_pred = nb_model.predict(X_ts)\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    precision = precision_score(y_test, y_pred, average='macro')\n",
    "    recall = recall_score(y_test, y_pred, average='macro')\n",
    "    f1 = f1_score(y_test, y_pred, average='macro')\n",
    "    \n",
    "    nb_results[feature_name] = {\n",
    "        'model': nb_model,\n",
    "        'accuracy': accuracy,\n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "        'f1': f1\n",
    "    }\n",
    "    \n",
    "    print(f\"  Accuracy: {accuracy:.4f} | F1 (macro): {f1:.4f}\\n\")\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"NAIVE BAYES - RESULTS SUMMARY\")\n",
    "print(\"=\"*70)\n",
    "for feature_name, metrics in nb_results.items():\n",
    "    print(f\"\\n{feature_name.upper()}:\")\n",
    "    print(f\"  Accuracy: {metrics['accuracy']:.4f}\")\n",
    "    print(f\"  Precision (macro): {metrics['precision']:.4f}\")\n",
    "    print(f\"  Recall (macro): {metrics['recall']:.4f}\")\n",
    "    print(f\"  F1-Score (macro): {metrics['f1']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "683a7ea7",
   "metadata": {},
   "source": [
    "## 7. Model 3: LSTM (Neural Network)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba302f0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, LSTM, Embedding, SpatialDropout1D, Bidirectional\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "print(\"Preparing data for LSTM...\\n\")\n",
    "\n",
    "# Tokenize text\n",
    "max_features = 5000\n",
    "max_length = 100\n",
    "\n",
    "tokenizer = Tokenizer(num_words=max_features)\n",
    "tokenizer.fit_on_texts(X_train_text)\n",
    "\n",
    "# Convert text to sequences\n",
    "X_train_seq = tokenizer.texts_to_sequences(X_train_text)\n",
    "X_dev_seq = tokenizer.texts_to_sequences(X_dev_text)\n",
    "X_test_seq = tokenizer.texts_to_sequences(X_test_text)\n",
    "\n",
    "# Pad sequences\n",
    "X_train_pad = pad_sequences(X_train_seq, maxlen=max_length)\n",
    "X_dev_pad = pad_sequences(X_dev_seq, maxlen=max_length)\n",
    "X_test_pad = pad_sequences(X_test_seq, maxlen=max_length)\n",
    "\n",
    "# Convert labels to binary (0 for ham, 1 for spam)\n",
    "y_train_bin = (y_train.values == 'spam').astype(int)\n",
    "y_dev_bin = (y_dev.values == 'spam').astype(int)\n",
    "y_test_bin = (y_test.values == 'spam').astype(int)\n",
    "\n",
    "print(f\"Train sequences: {X_train_pad.shape}\")\n",
    "print(f\"Dev sequences: {X_dev_pad.shape}\")\n",
    "print(f\"Test sequences: {X_test_pad.shape}\\n\")\n",
    "\n",
    "# Build LSTM model\n",
    "print(\"Building LSTM model...\")\n",
    "lstm_model = Sequential([\n",
    "    Embedding(max_features, 128),\n",
    "    SpatialDropout1D(0.2),\n",
    "    Bidirectional(LSTM(64, return_sequences=True)),\n",
    "    LSTM(32),\n",
    "    Dense(64, activation='relu'),\n",
    "    keras.layers.Dropout(0.5),\n",
    "    Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "lstm_model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train with early stopping\n",
    "print(\"Training LSTM model...\\n\")\n",
    "early_stop = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)\n",
    "history = lstm_model.fit(\n",
    "    X_train_pad, y_train_bin,\n",
    "    validation_data=(X_dev_pad, y_dev_bin),\n",
    "    epochs=15,\n",
    "    batch_size=32,\n",
    "    callbacks=[early_stop],\n",
    "    verbose=0\n",
    ")\n",
    "\n",
    "# Evaluate\n",
    "y_pred_lstm = (lstm_model.predict(X_test_pad, verbose=0) > 0.5).astype(int).flatten()\n",
    "accuracy_lstm = accuracy_score(y_test_bin, y_pred_lstm)\n",
    "precision_lstm = precision_score(y_test_bin, y_pred_lstm, average='macro')\n",
    "recall_lstm = recall_score(y_test_bin, y_pred_lstm, average='macro')\n",
    "f1_lstm = f1_score(y_test_bin, y_pred_lstm, average='macro')\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"LSTM - RESULTS\")\n",
    "print(\"=\"*70)\n",
    "print(f\"Accuracy: {accuracy_lstm:.4f}\")\n",
    "print(f\"Precision (macro): {precision_lstm:.4f}\")\n",
    "print(f\"Recall (macro): {recall_lstm:.4f}\")\n",
    "print(f\"F1-Score (macro): {f1_lstm:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb714346",
   "metadata": {},
   "source": [
    "## 8. Model 4: BERT (Transformer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "572fd05b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, TextClassificationPipeline\n",
    "import torch\n",
    "\n",
    "print(\"Loading BERT model...\\n\")\n",
    "\n",
    "# Use distilbert for faster inference\n",
    "model_name = \"distilbert-base-uncased\"\n",
    "tokenizer_bert = AutoTokenizer.from_pretrained(model_name)\n",
    "bert_model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=2)\n",
    "\n",
    "# Create pipeline\n",
    "bert_pipeline = TextClassificationPipeline(\n",
    "    model=bert_model, \n",
    "    tokenizer=tokenizer_bert, \n",
    "    device=0 if torch.cuda.is_available() else -1\n",
    ")\n",
    "\n",
    "print(\"Evaluating BERT on test set (sample)...\\n\")\n",
    "\n",
    "# Sample test data for speed\n",
    "sample_size = min(1000, len(X_test_text))\n",
    "sample_indices = np.random.choice(len(X_test_text), sample_size, replace=False)\n",
    "X_test_sample = X_test_text.iloc[sample_indices].values\n",
    "y_test_sample = y_test_bin[sample_indices]\n",
    "\n",
    "# Get predictions\n",
    "bert_predictions = []\n",
    "for i, text in enumerate(X_test_sample):\n",
    "    if i % 250 == 0:\n",
    "        print(f\"  Processed {i}/{len(X_test_sample)}\")\n",
    "    try:\n",
    "        result = bert_pipeline(text[:512], truncation=True)\n",
    "        label = 1 if result[0]['label'] == 'LABEL_1' else 0\n",
    "        bert_predictions.append(label)\n",
    "    except:\n",
    "        bert_predictions.append(0)\n",
    "\n",
    "bert_predictions = np.array(bert_predictions)\n",
    "\n",
    "# Evaluate\n",
    "accuracy_bert = accuracy_score(y_test_sample, bert_predictions)\n",
    "precision_bert = precision_score(y_test_sample, bert_predictions, average='macro', zero_division=0)\n",
    "recall_bert = recall_score(y_test_sample, bert_predictions, average='macro', zero_division=0)\n",
    "f1_bert = f1_score(y_test_sample, bert_predictions, average='macro', zero_division=0)\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"BERT - RESULTS (on sampled test set)\")\n",
    "print(\"=\"*70)\n",
    "print(f\"Accuracy: {accuracy_bert:.4f}\")\n",
    "print(f\"Precision (macro): {precision_bert:.4f}\")\n",
    "print(f\"Recall (macro): {recall_bert:.4f}\")\n",
    "print(f\"F1-Score (macro): {f1_bert:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9c804b2",
   "metadata": {},
   "source": [
    "## 9. Model 5: Gemma (via Ollama or HuggingFace)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51f8ad13",
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "\n",
    "print(\"Checking if Ollama is available...\\n\")\n",
    "\n",
    "# Test if Ollama is running\n",
    "try:\n",
    "    result = subprocess.run(['ollama', 'list'], capture_output=True, timeout=5, text=True)\n",
    "    if result.returncode == 0:\n",
    "        print(\"✓ Ollama is available\")\n",
    "        ollama_available = True\n",
    "        print(f\"\\nAvailable models:\\n{result.stdout}\")\n",
    "    else:\n",
    "        print(\"⚠ Ollama not accessible. Installation required.\")\n",
    "        ollama_available = False\n",
    "except:\n",
    "    print(\"⚠ Ollama not installed or not running\")\n",
    "    print(\"\\nTo use Ollama:\")\n",
    "    print(\"  1. Install from https://ollama.ai\")\n",
    "    print(\"  2. Run: ollama pull gemma\")\n",
    "    print(\"  3. Start Ollama service\")\n",
    "    ollama_available = False\n",
    "\n",
    "if ollama_available:\n",
    "    print(\"\\nRunning Ollama (Gemma) inference on sampled test set...\\n\")\n",
    "    # Sample a modest-sized subset for evaluation (adjustable)\n",
    "    sample_size = min(200, len(X_test_text))\n",
    "    sample_indices = np.random.choice(len(X_test_text), sample_size, replace=False)\n",
    "    sample_texts = X_test_text.iloc[sample_indices].values\n",
    "    y_test_sample = y_test_bin[sample_indices]  # binary labels (0=ham,1=spam)\n",
    "    ollama_predictions = []\n",
    "\n",
    "    # Use a model that appears in `ollama list` (e.g. gemma or gemma:7b)\n",
    "    ollama_model = \"gemma\"  # change to \"gemma:7b\" or another if preferred\n",
    "    timeout_sec = 600  # allow more time for cold start / model load\n",
    "\n",
    "    for i, text in enumerate(sample_texts):\n",
    "        if i % 50 == 0:\n",
    "            print(f\"  Processed {i}/{len(sample_texts)}\")\n",
    "        prompt = f\"Classify this email as SPAM or HAM (not spam). Answer with only SPAM or HAM.\\n\\nEmail: {text[:200]}\"\n",
    "        try:\n",
    "            result = subprocess.run(\n",
    "                ['ollama', 'run', ollama_model, prompt],\n",
    "                timeout=timeout_sec,\n",
    "                capture_output=True,\n",
    "                text=True,\n",
    "                input=\"\",\n",
    "            )\n",
    "\n",
    "            # If Ollama returns non-zero, surface stderr for debugging\n",
    "            if result.returncode != 0:\n",
    "                print(f\"  Ollama return code: {result.returncode}\")\n",
    "                if result.stderr:\n",
    "                    print(f\"  stderr: {result.stderr.strip()!r}\")\n",
    "\n",
    "            # Prefer stdout; fall back to stderr if stdout empty\n",
    "            output = (result.stdout or result.stderr or \"\").strip()\n",
    "            out_up = output.upper()\n",
    "            # Robust label extraction: look for a final standalone line 'SPAM' or 'HAM'\n",
    "            label = None\n",
    "            for line in reversed(output.splitlines()):\n",
    "                l = line.strip().upper()\n",
    "                if l in (\"SPAM\", \"HAM\"):\n",
    "                    label = l\n",
    "                    break\n",
    "                tokens = [t.strip(\" :.,\\\"'\\t\") for t in l.split()]\n",
    "                for tok in tokens:\n",
    "                    if tok in (\"SPAM\", \"HAM\"):\n",
    "                        label = tok\n",
    "                        break\n",
    "                if label:\n",
    "                    break\n",
    "\n",
    "            # Fallback: if no clear standalone token, use last occurrence in text\n",
    "            if label is None:\n",
    "                if \"SPAM\" in out_up:\n",
    "                    label = \"SPAM\"\n",
    "                elif \"HAM\" in out_up:\n",
    "                    label = \"HAM\"\n",
    "\n",
    "            pred = 1 if label == \"SPAM\" else 0\n",
    "            ollama_predictions.append(pred)\n",
    "        except subprocess.TimeoutExpired:\n",
    "            print(f\"  Error: Ollama command timed out after {timeout_sec} seconds\")\n",
    "            ollama_predictions.append(0)\n",
    "        except Exception as e:\n",
    "            print(f\"  Error: {str(e)}\")\n",
    "            ollama_predictions.append(0)\n",
    "\n",
    "    ollama_predictions = np.array(ollama_predictions)\n",
    "\n",
    "    # Compute metrics for Gemma\n",
    "    accuracy_gemma = accuracy_score(y_test_sample, ollama_predictions)\n",
    "    precision_gemma = precision_score(y_test_sample, ollama_predictions, average='macro', zero_division=0)\n",
    "    recall_gemma = recall_score(y_test_sample, ollama_predictions, average='macro', zero_division=0)\n",
    "    f1_gemma = f1_score(y_test_sample, ollama_predictions, average='macro', zero_division=0)\n",
    "\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"GEMMA (Ollama) - RESULTS (on sampled test set)\")\n",
    "    print(\"=\"*70)\n",
    "    print(f\"Accuracy: {accuracy_gemma:.4f}\")\n",
    "    print(f\"Precision (macro): {precision_gemma:.4f}\")\n",
    "    print(f\"Recall (macro): {recall_gemma:.4f}\")\n",
    "    print(f\"F1-Score (macro): {f1_gemma:.4f}\")\n",
    "    print(f\"\\n✓ Sample predictions: {['SPAM' if p==1 else 'HAM' for p in ollama_predictions[:20]]} (showing up to 20)\")\n",
    "\n",
    "    print(f\"\\nNote: Full evaluation skipped. Ollama inference is time-intensive.\")\n",
    "else:\n",
    "    print(\"\\nSkipping Ollama (Gemma) evaluation until service is available.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aff28e81",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "faa2ac5b",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23637f8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "\n",
    "print(\"Checking if Ollama is available...\\n\")\n",
    "\n",
    "# Test if Ollama is running\n",
    "try:\n",
    "    result = subprocess.run(['ollama', 'list'], capture_output=True, timeout=5, text=True)\n",
    "    if result.returncode == 0:\n",
    "        print(\"✓ Ollama is available\")\n",
    "        ollama_available = True\n",
    "        print(f\"\\nAvailable models:\\n{result.stdout}\")\n",
    "    else:\n",
    "        print(\"⚠ Ollama not accessible. Installation required.\")\n",
    "        ollama_available = False\n",
    "except:\n",
    "    print(\"⚠ Ollama not installed or not running\")\n",
    "    print(\"\\nTo use Ollama:\")\n",
    "    print(\"  1. Install from https://ollama.ai\")\n",
    "    print(\"  2. Run: ollama pull gemma\")\n",
    "    print(\"  3. Start Ollama service\")\n",
    "    ollama_available = False\n",
    "\n",
    "if ollama_available:\n",
    "    print(\"\\nRunning Ollama (GPT) inference on sampled test set...\\n\")\n",
    "    # Sample a modest-sized subset for evaluation (adjustable)\n",
    "    sample_size = min(200, len(X_test_text))\n",
    "    sample_indices = np.random.choice(len(X_test_text), sample_size, replace=False)\n",
    "    sample_texts = X_test_text.iloc[sample_indices].values\n",
    "    y_test_sample = y_test_bin[sample_indices]  # binary labels (0=ham,1=spam)\n",
    "    ollama_predictions = []\n",
    "\n",
    "    # Use a model that appears in `ollama list`\n",
    "    ollama_model = \"gpt-oss:20b\"\n",
    "    timeout_sec = 600  # allow more time for cold start / model load\n",
    "\n",
    "    for i, text in enumerate(sample_texts):\n",
    "        if i % 50 == 0:\n",
    "            print(f\"  Processed {i}/{len(sample_texts)}\")\n",
    "        prompt = f\"Classify this email as SPAM or HAM (not spam). Answer with only SPAM or HAM.\\n\\nEmail: {text[:200]}\"\n",
    "        try:\n",
    "            result = subprocess.run(\n",
    "                ['ollama', 'run', ollama_model, prompt],\n",
    "                timeout=timeout_sec,\n",
    "                capture_output=True,\n",
    "                text=True,\n",
    "                input=\"\",\n",
    "            )\n",
    "\n",
    "            # If Ollama returns non-zero, surface stderr for debugging\n",
    "            if result.returncode != 0:\n",
    "                print(f\"  Ollama return code: {result.returncode}\")\n",
    "                if result.stderr:\n",
    "                    print(f\"  stderr: {result.stderr.strip()!r}\")\n",
    "\n",
    "            # Prefer stdout; fall back to stderr if stdout empty\n",
    "            output = (result.stdout or result.stderr or \"\").strip()\n",
    "            out_up = output.upper()\n",
    "            # Robust label extraction: look for a final standalone line 'SPAM' or 'HAM'\n",
    "            label = None\n",
    "            for line in reversed(output.splitlines()):\n",
    "                l = line.strip().upper()\n",
    "                if l in (\"SPAM\", \"HAM\"):\n",
    "                    label = l\n",
    "                    break\n",
    "                tokens = [t.strip(\" :.,\\\"'\\t\") for t in l.split()]\n",
    "                for tok in tokens:\n",
    "                    if tok in (\"SPAM\", \"HAM\"):\n",
    "                        label = tok\n",
    "                        break\n",
    "                if label:\n",
    "                    break\n",
    "\n",
    "            # Fallback: if no clear standalone token, use last occurrence in text\n",
    "            if label is None:\n",
    "                if \"SPAM\" in out_up:\n",
    "                    label = \"SPAM\"\n",
    "                elif \"HAM\" in out_up:\n",
    "                    label = \"HAM\"\n",
    "\n",
    "            pred = 1 if label == \"SPAM\" else 0\n",
    "            ollama_predictions.append(pred)\n",
    "        except subprocess.TimeoutExpired:\n",
    "            print(f\"  Error: Ollama command timed out after {timeout_sec} seconds\")\n",
    "            ollama_predictions.append(0)\n",
    "        except Exception as e:\n",
    "            print(f\"  Error: {str(e)}\")\n",
    "            ollama_predictions.append(0)\n",
    "\n",
    "    ollama_predictions = np.array(ollama_predictions)\n",
    "\n",
    "    # Compute metrics for GPT model\n",
    "    accuracy_gpt = accuracy_score(y_test_sample, ollama_predictions)\n",
    "    precision_gpt = precision_score(y_test_sample, ollama_predictions, average='macro', zero_division=0)\n",
    "    recall_gpt = recall_score(y_test_sample, ollama_predictions, average='macro', zero_division=0)\n",
    "    f1_gpt = f1_score(y_test_sample, ollama_predictions, average='macro', zero_division=0)\n",
    "\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"GPT (Ollama) - RESULTS (on sampled test set)\")\n",
    "    print(\"=\"*70)\n",
    "    print(f\"Accuracy: {accuracy_gpt:.4f}\")\n",
    "    print(f\"Precision (macro): {precision_gpt:.4f}\")\n",
    "    print(f\"Recall (macro): {recall_gpt:.4f}\")\n",
    "    print(f\"F1-Score (macro): {f1_gpt:.4f}\")\n",
    "    print(f\"\\n✓ Sample predictions: {['SPAM' if p==1 else 'HAM' for p in ollama_predictions[:20]]} (showing up to 20)\")\n",
    "\n",
    "    print(f\"\\nNote: Full evaluation skipped. Ollama inference is time-intensive.\")\n",
    "else:\n",
    "    print(\"\\nSkipping Ollama (GPT) evaluation until service is available.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ac01008",
   "metadata": {},
   "source": [
    "## 11. Comprehensive Results Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f802f8be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile all results\n",
    "results_data = []\n",
    "\n",
    "# Logistic Regression results\n",
    "for feature_name, metrics in lr_results.items():\n",
    "    results_data.append({\n",
    "        'Model': 'Logistic Regression',\n",
    "        'Feature': feature_name.upper(),\n",
    "        'Accuracy': metrics['accuracy'],\n",
    "        'Precision': metrics['precision'],\n",
    "        'Recall': metrics['recall'],\n",
    "        'F1-Score': metrics['f1']\n",
    "    })\n",
    "\n",
    "# Naive Bayes results\n",
    "for feature_name, metrics in nb_results.items():\n",
    "    results_data.append({\n",
    "        'Model': 'Naive Bayes',\n",
    "        'Feature': feature_name.upper(),\n",
    "        'Accuracy': metrics['accuracy'],\n",
    "        'Precision': metrics['precision'],\n",
    "        'Recall': metrics['recall'],\n",
    "        'F1-Score': metrics['f1']\n",
    "    })\n",
    "\n",
    "# LSTM results\n",
    "results_data.append({\n",
    "    'Model': 'LSTM',\n",
    "    'Feature': 'Sequence Embedding',\n",
    "    'Accuracy': accuracy_lstm,\n",
    "    'Precision': precision_lstm,\n",
    "    'Recall': recall_lstm,\n",
    "    'F1-Score': f1_lstm\n",
    "})\n",
    "\n",
    "# BERT results\n",
    "results_data.append({\n",
    "    'Model': 'BERT',\n",
    "    'Feature': 'Transformer (Sampled)',\n",
    "    'Accuracy': accuracy_bert,\n",
    "    'Precision': precision_bert,\n",
    "    'Recall': recall_bert,\n",
    "    'F1-Score': f1_bert\n",
    "})\n",
    "\n",
    "# Add Gemma (Ollama) if metrics were computed\n",
    "if 'accuracy_gemma' in globals():\n",
    "    results_data.append({\n",
    "        'Model': 'Gemma (Ollama)',\n",
    "        'Feature': 'Gemma (Sampled)',\n",
    "        'Accuracy': accuracy_gemma,\n",
    "        'Precision': precision_gemma,\n",
    "        'Recall': recall_gemma,\n",
    "        'F1-Score': f1_gemma\n",
    "    })\n",
    "else:\n",
    "    print('Gemma metrics not available; skipping')\n",
    "\n",
    "# Add GPT (Ollama) if metrics were computed\n",
    "if 'accuracy_gpt' in globals():\n",
    "    results_data.append({\n",
    "        'Model': 'GPT (Ollama)',\n",
    "        'Feature': 'GPT (Sampled)',\n",
    "        'Accuracy': accuracy_gpt,\n",
    "        'Precision': precision_gpt,\n",
    "        'Recall': recall_gpt,\n",
    "        'F1-Score': f1_gpt\n",
    "    })\n",
    "else:\n",
    "    print('GPT metrics not available; skipping')\n",
    "\n",
    "# Create results dataframe\n",
    "results_df = pd.DataFrame(results_data)\n",
    "\n",
    "print(\"=\"*100)\n",
    "print(\"COMPREHENSIVE MODEL RESULTS COMPARISON\")\n",
    "print(\"=\"*100)\n",
    "print(f\"\\n{results_df.to_string(index=False)}\")\n",
    "\n",
    "# Best models\n",
    "print(\"\\n\" + \"=\"*100)\n",
    "print(\"TOP PERFORMING MODELS\")\n",
    "print(\"=\"*100)\n",
    "\n",
    "best_accuracy_idx = results_df['Accuracy'].idxmax()\n",
    "best_f1_idx = results_df['F1-Score'].idxmax()\n",
    "\n",
    "best_accuracy = results_df.loc[best_accuracy_idx]\n",
    "best_f1 = results_df.loc[best_f1_idx]\n",
    "\n",
    "print(f\"\\nBest Accuracy: {best_accuracy['Model']} ({best_accuracy['Feature']})\")\n",
    "print(f\"  → Accuracy: {best_accuracy['Accuracy']:.4f}\")\n",
    "\n",
    "print(f\"\\nBest F1-Score (macro): {best_f1['Model']} ({best_f1['Feature']})\")\n",
    "print(f\"  → F1-Score: {best_f1['F1-Score']:.4f}\")\n",
    "print(f\"  → Accuracy: {best_f1['Accuracy']:.4f}\")\n",
    "\n",
    "# Feature engineering comparison\n",
    "print(\"\\n\" + \"=\"*100)\n",
    "print(\"FEATURE ENGINEERING ANALYSIS\")\n",
    "print(\"=\"*100)\n",
    "\n",
    "for feature in ['UNIGRAM', 'BIGRAM', 'MIX', 'TFIDF']:\n",
    "    feature_results = results_df[results_df['Feature'] == feature]\n",
    "    if len(feature_results) > 0:\n",
    "        avg_accuracy = feature_results['Accuracy'].mean()\n",
    "        avg_f1 = feature_results['F1-Score'].mean()\n",
    "        print(f\"\\n{feature}:\")\n",
    "        print(f\"  Avg Accuracy: {avg_accuracy:.4f}\")\n",
    "        print(f\"  Avg F1-Score: {avg_f1:.4f}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*100)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
